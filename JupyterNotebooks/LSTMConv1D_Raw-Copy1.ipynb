{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e3cb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fde0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self, data_path, ann_path, list_files, list_ann_files, \n",
    "                 batch_size=64, dim=(3000,1), n_classes=5, shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.ann_path = ann_path\n",
    "        self.list_files = list_files\n",
    "        self.list_ann_files = list_ann_files\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.get_cnts() #Get the data count for each file        \n",
    "        self.on_epoch_end() #Initialize file indexes        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int((self.total_len+1) / self.batch_size)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        start = index*self.batch_size\n",
    "        end = min((index+1)*self.batch_size, self.total_len)\n",
    "        \n",
    "        X = np.empty((end - start,) + self.dim, dtype=np.float32)\n",
    "        y = np.empty((end - start,), dtype=np.int32)\n",
    "        \n",
    "        curr_file_idx, accum_start, accum_end = self.get_accum_idx(index)\n",
    "        \n",
    "        curr_file = self.list_files[self.file_indexes[curr_file_idx]]\n",
    "        curr_ann_file = self.list_ann_files[self.file_indexes[curr_file_idx]]\n",
    "        data_index = self.data_indexes[self.file_indexes[curr_file_idx]]\n",
    "        \n",
    "        curr_np = np.load(os.path.join(self.data_path, curr_file))\n",
    "        curr_ann = np.load(os.path.join(self.ann_path, curr_ann_file))\n",
    "        curr_np = curr_np[data_index]\n",
    "        curr_ann = curr_ann[data_index]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        X_1 = curr_np[start - accum_start:end - accum_start] \n",
    "        y_1 = curr_ann[start - accum_start:end - accum_start]\n",
    "        from_curr = min(accum_end - start, end - start)\n",
    "        X[:from_curr] = np.expand_dims(X_1, axis=-1)\n",
    "        y[:from_curr] = y_1\n",
    "        \n",
    "        if end > accum_end:\n",
    "            curr_file_idx += 1\n",
    "            accum_start = accum_end\n",
    "            accum_end += self.list_cnt[self.file_indexes[curr_file_idx]]\n",
    "            curr_file = self.list_files[self.file_indexes[curr_file_idx]]            \n",
    "            data_index = self.data_indexes[self.file_indexes[curr_file_idx]]\n",
    "            \n",
    "            \n",
    "            curr_ann_file = self.list_ann_files[self.file_indexes[curr_file_idx]]\n",
    "            curr_np = np.load(os.path.join(self.data_path, curr_file))\n",
    "            curr_ann = np.load(os.path.join(self.ann_path, curr_ann_file))\n",
    "\n",
    "            curr_np = curr_np[data_index]\n",
    "            curr_ann = curr_ann[data_index]\n",
    "            #curr_np = curr_np.reshape(-1, 3000, 1)\n",
    "            \n",
    "            #curr_np = curr_np[1:-1]\n",
    "            #curr_ann = curr_ann[1:-1]\n",
    "            \n",
    "            X_2 = curr_np[:end - accum_start]\n",
    "            y_2 = curr_ann[:end - accum_start]\n",
    "            X[from_curr:] = np.expand_dims(X_2, axis=-1)\n",
    "            y[from_curr:] = y_2\n",
    "        \n",
    "        '''\n",
    "        # Normalize data(MinMax)\n",
    "        rng = np.max(X, axis=1) - np.min(X, axis=1) #X shape: (B, 3000, 1), rng: (B, 1)\n",
    "        rng = np.expand_dims(rng, axis=1) #(B, 1, 1)\n",
    "        X = (X - np.expand_dims(np.min(X, axis=1),axis=1)) / (rng + 1e-8)\n",
    "        '''                \n",
    "        return X, y\n",
    "    \n",
    "    def get_accum_idx(self, index):\n",
    "        curr_file_idx = 0\n",
    "        accum_start = 0\n",
    "        accum_end = self.list_cnt[self.file_indexes[0]]\n",
    "        for i in range(len(self.file_indexes)):\n",
    "            if index * self.batch_size < accum_end:\n",
    "                curr_file_idx = i                \n",
    "                break            \n",
    "            accum_start += self.list_cnt[self.file_indexes[i]]\n",
    "            accum_end += self.list_cnt[self.file_indexes[i+1]]\n",
    "        \n",
    "        return curr_file_idx, accum_start, accum_end\n",
    "        \n",
    "    def on_epoch_end(self):        \n",
    "        self.curr_file_idx = 0\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.file_indexes = np.arange(len(self.list_files)) #This is necessary to shuffle files\n",
    "        self.data_indexes = [np.arange(cnt) for cnt in self.list_cnt]\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.file_indexes)\n",
    "            for i in range(len(self.list_cnt)):\n",
    "                np.random.shuffle(self.data_indexes[i]) \n",
    "            \n",
    "        #self.accum_start = 0 \n",
    "        #self.accum_end = self.list_cnt[self.file_indexes[0]]                 \n",
    "            \n",
    "    def get_cnts(self):\n",
    "        list_cnt = []\n",
    "        for f in self.list_files:\n",
    "            temp_np = np.load(os.path.join(self.data_path, f))\n",
    "            cnt_data = temp_np.shape[0] \n",
    "            list_cnt.append(cnt_data)\n",
    "            \n",
    "        self.list_cnt = list_cnt\n",
    "        self.total_len = sum(list_cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df9c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curr_path = os.getcwd() + '/'\n",
    "PROCESSED_DATA_PATH = os.path.join('/home','aiot','data','origin_npy')\n",
    "save_signals_path = os.path.join(PROCESSED_DATA_PATH,'signals_SC_filtered')\n",
    "save_annotations_path = os.path.join(PROCESSED_DATA_PATH,'annotations_SC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_annotations_npy(dirname, filename):\n",
    "    search_filename = filename.split('-')[0][:-2]\n",
    "    file_list = os.listdir(dirname)\n",
    "    filenames = [file for file in file_list if search_filename in file if file.endswith('.npy')]\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d64b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_HT1D = (3000,1)\n",
    "n_classes=6\n",
    "epochs = 50\n",
    "bs = 64\n",
    "BASE_LEARNING_RATE = 1e-3\n",
    "list_files = [f for f in os.listdir(save_signals_path) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b462b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_list(filepath):\n",
    "    import csv\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        list_filepath = [row[0] for row in spamreader]\n",
    "    return list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc54e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_train = os.path.join('/home','aiot','data','origin_npy','SC_train.csv')\n",
    "SC_test = os.path.join('/home','aiot','data','origin_npy','SC_test.csv')\n",
    "\n",
    "list_files_train = read_csv_to_list(SC_train)\n",
    "list_files_test = read_csv_to_list(SC_test)\n",
    "\n",
    "list_files_train = [f + '.npy' for f in list_files_train]\n",
    "list_files_test = [f + '.npy' for f in list_files_test]\n",
    "\n",
    "list_ann_files_train = []\n",
    "list_ann_files_test = []\n",
    "for f in list_files_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path, f)\n",
    "    list_ann_files_train.append(ann_file[0])\n",
    "    \n",
    "for f in list_files_test:\n",
    "    ann_file = match_annotations_npy(save_annotations_path, f)\n",
    "    list_ann_files_test.append(ann_file[0])\n",
    "\n",
    "\n",
    "\n",
    "#split_cnt = int(len(list_files) * 0.8)\n",
    "#list_files_train = list_files[:split_cnt]\n",
    "#list_files_test = list_files[split_cnt:]\n",
    "#list_ann_files_train = list_ann_files[:split_cnt]\n",
    "#list_ann_files_test = list_ann_files[split_cnt:]\n",
    "\n",
    "#list_files_train = list_files[:5]\n",
    "#list_files_test = list_files[80:90]\n",
    "#list_ann_files_train = list_ann_files[0:5]\n",
    "#list_ann_files_test = list_ann_files[80:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2528dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ac79f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(save_signals_path, save_annotations_path, list_files_train, list_ann_files_train, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5097eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(save_signals_path, save_annotations_path, list_files_test, list_ann_files_test, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758e45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "# Tested loss with class weight, but doesn't improve the accuracy\n",
    "\n",
    "from collections import defaultdict\n",
    "cnt_class = defaultdict(int)\n",
    "for x, y in train_generator:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for i, cnt in zip(unique, counts):\n",
    "        cnt_class[i] += cnt\n",
    "cnt_class_np = np.array(list(cnt_class.values()))\n",
    "class_weight = sum(cnt_class_np)/(n_classes * cnt_class_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cba8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, LSTM, Conv1D, ReLU, Input, Dense, Flatten, RepeatVector, Reshape, Dropout, add        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96881b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_heads, embed_dim):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.emb = Dense(embed_dim * n_heads * 3, use_bias=True)         \n",
    "        d = tf.cast(embed_dim, dtype=tf.float32)    \n",
    "        self.scaling = 1/tf.math.sqrt(d)\n",
    "\n",
    "    def call(self, inputs):    \n",
    "        \"\"\"\n",
    "        inputs: (B, num_seed, features)\n",
    "        \"\"\"\n",
    "        #num_seed = tf.shape(inputs)[1]\n",
    "        embedding = self.emb(inputs) # (B, n, d * h * 3)            \n",
    "        heads = Reshape((-1, self.embed_dim, self.n_heads, 3))(embedding) #(B, n, d, h, 3)\n",
    "        \n",
    "\n",
    "        heads = tf.transpose(heads, perm=[0,4,3,1,2]) # (B, 3, h, n, d)\n",
    "        q = heads[:,0,:,:,:] #(B, h, n, d)\n",
    "        k = heads[:,1,:,:,:] #(B, h, n, d)\n",
    "        v = heads[:,2,:,:,:] #(B, h, n, d)\n",
    "        \n",
    "        qk = tf.matmul(q, k, transpose_b=True) # (B, h, n, n)    \n",
    "        qk = tf.keras.backend.softmax(qk) * self.scaling # (B, h, n, n)            \n",
    "        attn = qk / self.scaling\n",
    "        \n",
    "        output = tf.matmul(attn, v) # (B, h, n, d)\n",
    "        output = tf.transpose(output, perm=[0,2,1,3]) #(B, n, h, d)\n",
    "        output = Reshape((-1, self.embed_dim * self.n_heads))(output)\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c32826a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv1d_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters=64, kernel_size=100, strides=1, padding='valid'):\n",
    "        super().__init__()\n",
    "        self.conv = Conv1D(filters=filters, kernel_size=kernel_size, padding=padding, strides=strides)\n",
    "        self.bn = BatchNormalization(axis=-1)\n",
    "        self.relu = ReLU()\n",
    "    def call(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cdd7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttention(tf.keras.Model):\n",
    "    def __init__(self, input_shape=(3000,1), n_classes=6):\n",
    "        super().__init__()\n",
    "        #self.input = Input(shape=input_shape)\n",
    "        \n",
    "        self.conv0_1 = conv1d_block(filters=32, kernel_size=300, strides=10)\n",
    "        self.conv0_2 = conv1d_block(filters=64, kernel_size=5, strides=3)\n",
    "        #self.squeeze_conv1 = conv1d_block(filters=64, kernel_size=179, strides=1)\n",
    "        #self.squeeze_conv2 = conv1d_block(filters=256, kernel_size=1, strides=1)\n",
    "        self.lstm1 = LSTM(32)\n",
    "        self.dense1 = Dense(256)\n",
    "        self.mha1 = MultiheadAttention(n_heads=8, embed_dim=32)\n",
    "        self.conv2_1 = conv1d_block(filters=256, kernel_size=1, strides=1, padding='same')\n",
    "        self.conv2_2 = conv1d_block(filters=256, kernel_size=1, strides=1, padding='same')\n",
    "        self.conv2_3 = conv1d_block(filters=256, kernel_size=1, strides=1, padding='same')\n",
    "        self.dropout2_1 = Dropout(0.1)\n",
    "        \n",
    "        #self.mha2 = MultiheadAttention(n_heads=8, embed_dim=32)\n",
    "        #self.conv2_4 = conv1d_block(filters=512, kernel_size=1, strides=1, padding='same')\n",
    "        #self.conv2_5 = conv1d_block(filters=512, kernel_size=1, strides=1, padding='same')\n",
    "        #self.conv2_6 = conv1d_block(filters=512, kernel_size=1, strides=1, padding='same')        \n",
    "        \n",
    "        self.conv1_1 = conv1d_block(filters=128, kernel_size=5, strides=2)\n",
    "        self.conv1_2 = conv1d_block(filters=128, kernel_size=5, strides=1, padding='same')\n",
    "        self.conv1_3 = conv1d_block(filters=128, kernel_size=5, strides=1, padding='same')\n",
    "        self.dropout1_1 = Dropout(0.1)\n",
    "        \n",
    "        self.conv1_4 = conv1d_block(filters=256, kernel_size=5, strides=2)\n",
    "        self.conv1_5 = conv1d_block(filters=256, kernel_size=5, strides=1, padding='same')\n",
    "        self.conv1_6 = conv1d_block(filters=256, kernel_size=5, strides=1, padding='same')\n",
    "        self.dropout1_2 = Dropout(0.1)\n",
    "                \n",
    "        self.dropout3 = Dropout(0.1)\n",
    "        self.final_conv1 = conv1d_block(filters=256, kernel_size=5, strides=1, padding='same')\n",
    "        self.final_conv2 = conv1d_block(filters=256, kernel_size=5, strides=1, padding='same')\n",
    "        self.final_conv3 = conv1d_block(filters=256, kernel_size=5, strides=1, padding='same')\n",
    "        \n",
    "        self.fc = Dense(n_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, x):\n",
    "        #x = self.input(x)        \n",
    "        x = self.conv0_1(x)\n",
    "        \n",
    "        y = self.lstm1(x)\n",
    "        y = self.dense1(y)\n",
    "        y = Reshape((1,1,256))(y)\n",
    "        identity = y\n",
    "        y = self.mha1(y)\n",
    "        y = self.conv2_1(y)\n",
    "        y = self.conv2_2(y)\n",
    "        y = self.conv2_3(y)\n",
    "        y = self.dropout2_1(y)\n",
    "        y = add([identity, y])\n",
    "        \n",
    "        \n",
    "        x = self.conv0_2(x)\n",
    "        #y = self.squeeze_conv1(x)\n",
    "        #y = self.squeeze_conv2(y)       \n",
    "        \n",
    "        x = self.conv1_1(x)\n",
    "        identity = x\n",
    "        x = self.conv1_2(x)\n",
    "        x = self.conv1_3(x)\n",
    "        x = self.dropout1_1(x)\n",
    "        x = add([identity, x])\n",
    "        \n",
    "        x = self.conv1_4(x)\n",
    "        identity = x\n",
    "        x = self.conv1_5(x)\n",
    "        x = self.conv1_6(x)\n",
    "        x = self.dropout1_2(x)\n",
    "        x = add([identity, x])\n",
    "        \n",
    "        \n",
    "        y = Reshape((256,))(y)\n",
    "        y = RepeatVector(20)(y)\n",
    "        \n",
    "        x = add([x, y])\n",
    "        x = self.dropout3(x)\n",
    "               \n",
    "        identity = x\n",
    "        x = self.final_conv1(x)        \n",
    "        x = self.final_conv2(x)        \n",
    "        x = self.final_conv3(x) \n",
    "        x = add([identity, x])\n",
    "        x = Flatten()(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bcb59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47dd3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.16747847, 0.165163  , 0.17343563, 0.15941907, 0.16582629,\n",
       "        0.16867754]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1,3000,1))\n",
    "x = tf.convert_to_tensor(x)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f391259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_attention_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_block_14 (conv1d_bloc multiple                  9760      \n",
      "_________________________________________________________________\n",
      "conv1d_block_15 (conv1d_bloc multiple                  10560     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8320      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  8448      \n",
      "_________________________________________________________________\n",
      "multihead_attention_1 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "conv1d_block_16 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_17 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_18 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_block_19 (conv1d_bloc multiple                  41600     \n",
      "_________________________________________________________________\n",
      "conv1d_block_20 (conv1d_bloc multiple                  82560     \n",
      "_________________________________________________________________\n",
      "conv1d_block_21 (conv1d_bloc multiple                  82560     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_block_22 (conv1d_bloc multiple                  165120    \n",
      "_________________________________________________________________\n",
      "conv1d_block_23 (conv1d_bloc multiple                  328960    \n",
      "_________________________________________________________________\n",
      "conv1d_block_24 (conv1d_bloc multiple                  328960    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_block_25 (conv1d_bloc multiple                  328960    \n",
      "_________________________________________________________________\n",
      "conv1d_block_26 (conv1d_bloc multiple                  328960    \n",
      "_________________________________________________________________\n",
      "conv1d_block_27 (conv1d_bloc multiple                  328960    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  30726     \n",
      "=================================================================\n",
      "Total params: 2,482,278\n",
      "Trainable params: 2,476,710\n",
      "Non-trainable params: 5,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e79fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = BASE_LEARNING_RATE\n",
    "    for _ in range(epoch // 10):\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    optimizer.learning_rate = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53898c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        bs = y_pred.shape[0]\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        #loss = -K.sum(loss, -1)\n",
    "        loss = -K.sum(loss) / bs\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "035a46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "#loss_fn = weighted_categorical_crossentropy(weights=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40130015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpt_Advanced_Conv1D', max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ed5b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "#if manager.latest_checkpoint:\n",
    "#    ckpt.restore(manager.latest_checkpoint)\n",
    "#    start_epoch = ckpt.step.numpy()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f75d2271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 0--------------------\n",
      "[1160 / 1162] Training loss: 0.610771, Training acc: 0.823\n",
      "Training time: 606.03 sec \n",
      "[510 / 515] test loss: 1.595881, test accuracy: 0.755\n",
      "Eval time: 99.09 sec\n",
      "Saved checkpoint for step 2: ./ckpt_Advanced_Conv1D/ckpt-1\n",
      "-------------------- Epoch 1--------------------\n",
      "[1160 / 1162] Training loss: 0.305751, Training acc: 0.900\n",
      "Training time: 605.64 sec \n",
      "[510 / 515] test loss: 1.143505, test accuracy: 0.794\n",
      "Eval time: 99.35 sec\n",
      "Saved checkpoint for step 3: ./ckpt_Advanced_Conv1D/ckpt-2\n",
      "-------------------- Epoch 2--------------------\n",
      "[1160 / 1162] Training loss: 0.236334, Training acc: 0.919\n",
      "Training time: 606.34 sec \n",
      "[510 / 515] test loss: 1.247552, test accuracy: 0.813\n",
      "Eval time: 99.47 sec\n",
      "Saved checkpoint for step 4: ./ckpt_Advanced_Conv1D/ckpt-3\n",
      "-------------------- Epoch 3--------------------\n",
      "[1160 / 1162] Training loss: 0.217462, Training acc: 0.924\n",
      "Training time: 605.93 sec \n",
      "[510 / 515] test loss: 1.528251, test accuracy: 0.797\n",
      "Eval time: 99.83 sec\n",
      "Saved checkpoint for step 5: ./ckpt_Advanced_Conv1D/ckpt-4\n",
      "-------------------- Epoch 4--------------------\n",
      "[1160 / 1162] Training loss: 0.194125, Training acc: 0.931\n",
      "Training time: 605.92 sec \n",
      "[510 / 515] test loss: 1.626669, test accuracy: 0.787\n",
      "Eval time: 99.74 sec\n",
      "Saved checkpoint for step 6: ./ckpt_Advanced_Conv1D/ckpt-5\n",
      "-------------------- Epoch 5--------------------\n",
      "[1160 / 1162] Training loss: 0.177156, Training acc: 0.937\n",
      "Training time: 609.21 sec \n",
      "[510 / 515] test loss: 1.761907, test accuracy: 0.728\n",
      "Eval time: 99.59 sec\n",
      "Saved checkpoint for step 7: ./ckpt_Advanced_Conv1D/ckpt-6\n",
      "-------------------- Epoch 6--------------------\n",
      "[1160 / 1162] Training loss: 0.156486, Training acc: 0.943\n",
      "Training time: 605.49 sec \n",
      "[510 / 515] test loss: 1.457932, test accuracy: 0.823\n",
      "Eval time: 99.53 sec\n",
      "Saved checkpoint for step 8: ./ckpt_Advanced_Conv1D/ckpt-7\n",
      "-------------------- Epoch 7--------------------\n",
      "[1160 / 1162] Training loss: 0.129967, Training acc: 0.953\n",
      "Training time: 605.96 sec \n",
      "[510 / 515] test loss: 1.737667, test accuracy: 0.845\n",
      "Eval time: 99.74 sec\n",
      "Saved checkpoint for step 9: ./ckpt_Advanced_Conv1D/ckpt-8\n",
      "-------------------- Epoch 8--------------------\n",
      "[1160 / 1162] Training loss: 0.125620, Training acc: 0.956\n",
      "Training time: 606.47 sec \n",
      "[510 / 515] test loss: 1.777808, test accuracy: 0.824\n",
      "Eval time: 99.21 sec\n",
      "Saved checkpoint for step 10: ./ckpt_Advanced_Conv1D/ckpt-9\n",
      "-------------------- Epoch 9--------------------\n",
      "[1160 / 1162] Training loss: 0.091955, Training acc: 0.966\n",
      "Training time: 605.79 sec \n",
      "[510 / 515] test loss: 1.315451, test accuracy: 0.850\n",
      "Eval time: 99.75 sec\n",
      "Saved checkpoint for step 11: ./ckpt_Advanced_Conv1D/ckpt-10\n",
      "-------------------- Epoch 10--------------------\n",
      "[1160 / 1162] Training loss: 0.103660, Training acc: 0.963\n",
      "Training time: 606.23 sec \n",
      "[510 / 515] test loss: 1.573748, test accuracy: 0.851\n",
      "Eval time: 99.72 sec\n",
      "Saved checkpoint for step 12: ./ckpt_Advanced_Conv1D/ckpt-11\n",
      "-------------------- Epoch 11--------------------\n",
      "[1160 / 1162] Training loss: 0.065580, Training acc: 0.976\n",
      "Training time: 606.57 sec \n",
      "[510 / 515] test loss: 1.660751, test accuracy: 0.852\n",
      "Eval time: 99.41 sec\n",
      "Saved checkpoint for step 13: ./ckpt_Advanced_Conv1D/ckpt-12\n",
      "-------------------- Epoch 12--------------------\n",
      "[1160 / 1162] Training loss: 0.048070, Training acc: 0.984\n",
      "Training time: 605.32 sec \n",
      "[510 / 515] test loss: 1.719103, test accuracy: 0.852\n",
      "Eval time: 99.38 sec\n",
      "Saved checkpoint for step 14: ./ckpt_Advanced_Conv1D/ckpt-13\n",
      "-------------------- Epoch 13--------------------\n",
      "[1160 / 1162] Training loss: 0.034692, Training acc: 0.990\n",
      "Training time: 604.30 sec \n",
      "[510 / 515] test loss: 1.743196, test accuracy: 0.854\n",
      "Eval time: 99.10 sec\n",
      "Saved checkpoint for step 15: ./ckpt_Advanced_Conv1D/ckpt-14\n",
      "-------------------- Epoch 14--------------------\n",
      "[1160 / 1162] Training loss: 0.026393, Training acc: 0.993\n",
      "Training time: 604.47 sec \n",
      "[510 / 515] test loss: 1.969210, test accuracy: 0.853\n",
      "Eval time: 99.18 sec\n",
      "Saved checkpoint for step 16: ./ckpt_Advanced_Conv1D/ckpt-15\n",
      "-------------------- Epoch 15--------------------\n",
      "[1160 / 1162] Training loss: 0.019217, Training acc: 0.996\n",
      "Training time: 604.80 sec \n",
      "[510 / 515] test loss: 2.009061, test accuracy: 0.857\n",
      "Eval time: 99.33 sec\n",
      "Saved checkpoint for step 17: ./ckpt_Advanced_Conv1D/ckpt-16\n",
      "-------------------- Epoch 16--------------------\n",
      "[1160 / 1162] Training loss: 0.015036, Training acc: 0.997\n",
      "Training time: 604.84 sec \n",
      "[510 / 515] test loss: 1.961347, test accuracy: 0.855\n",
      "Eval time: 99.26 sec\n",
      "Saved checkpoint for step 18: ./ckpt_Advanced_Conv1D/ckpt-17\n",
      "-------------------- Epoch 17--------------------\n",
      "[1160 / 1162] Training loss: 0.011324, Training acc: 0.999\n",
      "Training time: 605.02 sec \n",
      "[510 / 515] test loss: 2.012617, test accuracy: 0.856\n",
      "Eval time: 99.12 sec\n",
      "Saved checkpoint for step 19: ./ckpt_Advanced_Conv1D/ckpt-18\n",
      "-------------------- Epoch 18--------------------\n",
      "[1160 / 1162] Training loss: 0.008686, Training acc: 0.999\n",
      "Training time: 603.89 sec \n",
      "[510 / 515] test loss: 2.119868, test accuracy: 0.856\n",
      "Eval time: 98.96 sec\n",
      "Saved checkpoint for step 20: ./ckpt_Advanced_Conv1D/ckpt-19\n",
      "-------------------- Epoch 19--------------------\n",
      "[1160 / 1162] Training loss: 0.007452, Training acc: 0.999\n",
      "Training time: 604.03 sec \n",
      "[510 / 515] test loss: 2.265564, test accuracy: 0.856\n",
      "Eval time: 99.01 sec\n",
      "Saved checkpoint for step 21: ./ckpt_Advanced_Conv1D/ckpt-20\n",
      "-------------------- Epoch 20--------------------\n",
      "[1160 / 1162] Training loss: 0.014225, Training acc: 0.996\n",
      "Training time: 604.06 sec \n",
      "[510 / 515] test loss: 2.173931, test accuracy: 0.858\n",
      "Eval time: 98.84 sec\n",
      "Saved checkpoint for step 22: ./ckpt_Advanced_Conv1D/ckpt-21\n",
      "-------------------- Epoch 21--------------------\n",
      "[1160 / 1162] Training loss: 0.011735, Training acc: 0.997\n",
      "Training time: 604.37 sec \n",
      "[510 / 515] test loss: 2.157518, test accuracy: 0.859\n",
      "Eval time: 99.19 sec\n",
      "Saved checkpoint for step 23: ./ckpt_Advanced_Conv1D/ckpt-22\n",
      "-------------------- Epoch 22--------------------\n",
      "[1160 / 1162] Training loss: 0.010658, Training acc: 0.997\n",
      "Training time: 604.95 sec \n",
      "[510 / 515] test loss: 2.162785, test accuracy: 0.859\n",
      "Eval time: 99.33 sec\n",
      "Saved checkpoint for step 24: ./ckpt_Advanced_Conv1D/ckpt-23\n",
      "-------------------- Epoch 23--------------------\n",
      "[540 / 1162] Training loss: 0.011017, Training acc: 0.997\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-b351c7fe9b7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-c7f2286545c5>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv0_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 **gpu_lstm_kwargs)\n\u001b[1;32m   1248\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m             last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 **normal_lstm_kwargs)\n\u001b[1;32m   1251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[0;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   1377\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m   last_output, outputs, new_states = backend.rnn(\n\u001b[0m\u001b[1;32m   1380\u001b[0m       \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minit_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   4497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4499\u001b[0;31m       final_outputs = tf.compat.v1.while_loop(\n\u001b[0m\u001b[1;32m   4500\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4501\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2775\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   4483\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4484\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4485\u001b[0;31m         output, new_states = step_function(current_input,\n\u001b[0m\u001b[1;32m   4486\u001b[0m                                            tuple(states) + tuple(constants))\n\u001b[1;32m   4487\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_tm1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m     \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    528\u001b[0m   \"\"\"\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6230\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6231\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6232\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   6233\u001b[0m         _ctx, \"Mul\", name, x, y)\n\u001b[1;32m   6234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_acc = 0.0\n",
    "for e in range(start_epoch, epochs):\n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    print('-'*20, 'Epoch ' + str(e) + '-'*20)\n",
    "    adjust_learning_rate(optimizer, e)\n",
    "    start = time.time()\n",
    "    for idx, (x, y) in enumerate(train_generator): \n",
    "        #y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            #loss = loss_fn(y_onehot, y_pred)\n",
    "            loss = loss_fn(y, y_pred)\n",
    "        \n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_loss += loss * y_pred.shape[0]\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(\"[%d / %d] Training loss: %.6f, Training acc: %.3f\"%\n",
    "                  (idx+1, len(train_generator), total_loss / total_cnt, correct / total_cnt),end='\\r', flush=True)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    print(\"\")\n",
    "    print(\"Training time: %.2f sec \"%(time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    for idx, (x, y) in enumerate(test_generator):\n",
    "        y_pred = model(x, training=False)\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        #total_loss += loss_fn(y_onehot, y_pred).numpy() * y_pred.shape[0]\n",
    "        total_loss += loss_fn(y, y_pred).numpy() * y_pred.shape[0]\n",
    "            \n",
    "        test_acc = correct / total_cnt\n",
    "        test_loss = total_loss / total_cnt\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(\"[%d / %d] test loss: %.6f, test accuracy: %.3f\"%\n",
    "                  (idx+1, len(test_generator), test_loss, test_acc),end='\\r', flush=True)\n",
    "    print(\"\")\n",
    "    print(\"Eval time: %.2f sec\"%(time.time() - start))\n",
    "    ckpt.step.assign_add(1)\n",
    "    if test_acc > best_test_acc:\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
