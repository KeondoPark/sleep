{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e3cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-03 14:12:01.909417: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64ee9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6a67bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data.datagen' from '/home/keondopark/sleep/JupyterNotebooks/../Data/datagen.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nets\n",
    "from Data import datagen\n",
    "import importlib \n",
    "import resnet1D_Ahmed\n",
    "importlib.reload(datagen)  # Python 3.4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a9b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df9c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = os.path.join('/home','aiot','data','origin_npy')\n",
    "save_signals_path_SC = os.path.join(PROCESSED_DATA_PATH,'signals_SC_filtered')\n",
    "save_annotations_path_SC = os.path.join(PROCESSED_DATA_PATH,'annotations_SC')\n",
    "save_signals_path_ST = os.path.join(PROCESSED_DATA_PATH,'signals_ST_filtered')\n",
    "save_annotations_path_ST = os.path.join(PROCESSED_DATA_PATH,'annotations_ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_annotations_npy(dirname, filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    search_filename = filename.split('-')[0][:-2]\n",
    "    file_list = os.listdir(dirname)\n",
    "    filenames = [file for file in file_list if search_filename in file if file.endswith('.npy')]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d64b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_HT1D = (3000,1)\n",
    "n_classes=6\n",
    "epochs = 50\n",
    "bs = 64\n",
    "BASE_LEARNING_RATE = 1e-3\n",
    "data_ratio = 0.25\n",
    "PREV_CNT = 10\n",
    "list_files_SC = [os.path.join(save_signals_path_SC, f) for f in os.listdir(save_signals_path_SC) if f.endswith('.npy')]\n",
    "list_files_ST = [os.path.join(save_signals_path_ST, f) for f in os.listdir(save_signals_path_ST) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf21d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_list(filepath):\n",
    "    import csv\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        list_filepath = [row[0] for row in spamreader]\n",
    "    return list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccc54e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.7\n",
    "split_cnt_SC = int(train_test_split * len(list_files_SC))\n",
    "split_cnt_ST = int(train_test_split * len(list_files_ST))\n",
    "\n",
    "list_files_train = []\n",
    "list_files_test = []\n",
    "\n",
    "list_ann_files_train = []\n",
    "list_ann_files_test = []\n",
    "\n",
    "\n",
    "list_files_SC_train = np.random.choice(list_files_SC[:split_cnt_SC], int(0.25 * split_cnt_SC), replace=False)\n",
    "list_files_train += list_files_SC_train.tolist()\n",
    "for f in list_files_SC_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_SC, f)\n",
    "    list_ann_files_train.append(os.path.join(save_annotations_path_SC, ann_file[0]))\n",
    "\n",
    "list_files_test += list_files_SC[split_cnt_SC:]\n",
    "\n",
    "for f in list_files_SC[split_cnt_SC:]:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_SC, f)\n",
    "    list_ann_files_test.append(os.path.join(save_annotations_path_SC, ann_file[0]))\n",
    "\n",
    "\n",
    "list_files_ST_train = np.random.choice(list_files_ST[:split_cnt_ST], int(0.25 * split_cnt_ST), replace=False)\n",
    "list_files_train += list_files_ST_train.tolist()\n",
    "for f in list_files_ST_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_ST, f)\n",
    "    list_ann_files_train.append(os.path.join(save_annotations_path_ST, ann_file[0]))\n",
    "\n",
    "list_files_test += list_files_ST[split_cnt_ST:]\n",
    "for f in list_files_ST[split_cnt_ST:]:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_ST, f)\n",
    "    list_ann_files_test.append(os.path.join(save_annotations_path_ST, ann_file[0]))\n",
    "\n",
    "train_generator = datagen.DataGenerator(list_files_train, list_ann_files_train, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False, balanced_sampling=True)\n",
    "test_generator = datagen.DataGenerator(list_files_test, list_ann_files_test, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False, balanced_sampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afac03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = np.load(list_ann_files_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c981da76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2694"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1528126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in train_generator.data_indexes[0]:\n",
    "    print(first_file[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "967c47c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1080, 2766,  551, ..., 2381, 1896, 1835]),\n",
       " array([  43,  324,  400, ..., 1244, 1286, 1255]),\n",
       " array([2153,   81, 2165, ..., 1154,  975,  971]),\n",
       " array([2329, 2377, 1911, ..., 1769, 1745, 1578]),\n",
       " array([ 556, 2395,  248, ..., 1697,  901,  876]),\n",
       " array([ 732,   51,   12, ..., 1662, 1827, 1264]),\n",
       " array([ 650,  128, 2613, ..., 1112, 1130, 1419]),\n",
       " array([2766, 2543, 2292, ..., 1060, 1066, 1320]),\n",
       " array([2436, 2596,  779, ..., 1099, 1775, 1811]),\n",
       " array([2455, 1481,  587, ..., 2065, 2138, 1766]),\n",
       " array([1949, 2146,  140, ..., 1531, 1046, 1203]),\n",
       " array([1784, 2588, 2464, ..., 1691, 1203, 1228]),\n",
       " array([ 179, 1971, 2109, ..., 1730, 1445, 1119]),\n",
       " array([1400, 2206, 2058, ..., 1903, 1897, 1636]),\n",
       " array([2029, 1884,  907, ..., 1065, 1037, 1370]),\n",
       " array([ 777, 2145,  646, ..., 1319, 1406, 1485]),\n",
       " array([ 838, 2455,  279, ..., 1847, 1425, 1386]),\n",
       " array([ 587, 1868,  240, ..., 1145, 1164, 1159]),\n",
       " array([ 631, 2453, 1287, ..., 1415, 1416, 1417]),\n",
       " array([ 568, 2075,  610, ..., 1763, 1328, 1478]),\n",
       " array([ 322, 2101,    2, ..., 1705, 1363, 1366]),\n",
       " array([2304, 2280, 2276, ..., 1345, 1330, 1331]),\n",
       " array([ 319, 1978, 2656, ..., 1595, 1409, 1453]),\n",
       " array([ 890, 2053,  452, ..., 1612, 1634, 1784]),\n",
       " array([1859, 1903, 2586, ..., 1304, 1717, 1536]),\n",
       " array([2139, 2285, 1892, ..., 1388, 1570, 1746]),\n",
       " array([  6, 566, 201, ..., 738, 433, 721]),\n",
       " array([476, 843, 862, ..., 239, 710, 719]),\n",
       " array([865, 944, 277,   9, 933, 886, 945,  10, 886,  12, 886, 168, 936,\n",
       "        898,   2, 903, 946,   6, 866, 881, 949,  14, 892, 878, 900, 892,\n",
       "        947, 183,  18,   6, 936, 949, 818, 941,   4, 899, 169, 866, 277,\n",
       "          5,  14, 933, 903, 881, 550,   7,   4, 887, 899, 890, 884,  11,\n",
       "        884,   0, 575, 577, 888, 882, 277, 945, 896, 575, 904,   3,  11,\n",
       "         19, 894, 892,  16, 818, 577, 938, 945, 737, 940, 899, 718, 935,\n",
       "        866, 944, 885, 906,   5, 819,   1, 890,   5, 866, 866, 905, 938,\n",
       "         26, 442, 890, 718, 935, 718, 886,   3, 901, 936, 897, 947, 737,\n",
       "        901, 945,   7, 884, 892, 865, 819,  18,   0,   0, 901, 945, 885,\n",
       "          0, 880, 224, 891,   0, 878,  11, 888,   4,  12, 819, 948,   9,\n",
       "         14, 947, 883,   0,   9, 881, 865,  16, 169, 934, 895,  15, 550,\n",
       "        894, 935,  12,   9, 941, 948, 906,  90,  13, 940, 941, 898, 550,\n",
       "        902,   6, 904, 575, 893, 945,  90,   5,  13,  16, 883, 224, 819,\n",
       "        883, 906,  11, 884, 575, 277, 941, 819, 500,   4,  17, 737,  13,\n",
       "        550, 866, 936, 891, 277,  17, 941, 891, 700, 930, 206, 611,  28,\n",
       "        486, 939, 692,  29, 541, 486, 920, 557, 237,  34, 825, 236,  39,\n",
       "        213, 492, 914,  81, 657, 185, 909, 262, 557, 272, 932, 924, 747,\n",
       "        925, 264, 919, 595, 922, 541,  24, 869, 264, 335, 926, 576, 921,\n",
       "        583, 213, 181, 180, 731, 928, 201, 344, 834, 184,  40,  25,  21,\n",
       "        208, 700,  24, 661, 697, 558, 909,  28, 426, 908,  25, 927, 580,\n",
       "        656, 234, 867, 580,  91,  34, 915, 209, 917, 879, 927, 741, 200,\n",
       "        489, 170, 738, 825,  22, 914, 491, 915, 181, 923, 530, 721, 927,\n",
       "        551, 747, 660, 541, 871, 279, 915, 594,  28, 489,  36, 820, 719,\n",
       "        730, 216, 911, 555, 487,  40, 220, 909, 268, 939, 214, 825, 697,\n",
       "        913, 872,  37, 922, 740, 208, 186, 578, 581, 485, 269, 731, 839,\n",
       "        201, 918, 922, 488, 744, 178, 580, 186, 742, 872, 216, 921,  36,\n",
       "        870, 309, 485, 741,  33, 738, 172, 225, 181, 213, 697,  24, 907,\n",
       "        491, 928, 266, 485, 580, 173, 870, 928, 278, 939, 823, 834, 741,\n",
       "        207, 700, 656, 177, 700, 870, 943, 220, 269, 177, 226, 727,  91,\n",
       "        201,  24, 205, 791, 450, 255, 678, 673, 395, 437, 763, 462, 370,\n",
       "        750, 360, 608, 405, 124, 399, 253, 792, 628, 371, 601, 465, 666,\n",
       "        447, 378, 125, 276, 365, 447,  95, 431, 130, 831, 104, 814, 252,\n",
       "        477, 439, 219, 633,  60,  53, 655, 281, 110, 368,  43, 761, 368,\n",
       "        652, 383, 814, 379, 418, 362, 411,  92, 440, 405, 503, 483, 392,\n",
       "        802, 790, 363, 483, 642, 507, 351, 769, 647, 379, 628,  63, 445,\n",
       "        276, 423, 619,  43, 748, 484, 807, 449, 131, 238, 419, 592, 767,\n",
       "        248,  46, 410, 601, 778, 466, 463, 653, 613, 194, 807, 195, 121,\n",
       "         45, 748, 636, 493, 117, 233, 115, 608, 506, 771, 810,  82, 788,\n",
       "        223, 244, 610, 641, 245, 218, 662,  73, 621, 504,  80, 417, 467,\n",
       "        638, 389, 422, 377, 470, 754,  70, 249, 377, 826, 456, 122, 637,\n",
       "        625, 352, 413, 252, 460,  54, 446, 416, 118, 403,  43, 750, 219,\n",
       "        415, 772, 503,  68, 276, 494, 255, 360, 381,  56, 440, 398, 391,\n",
       "        631, 477, 421, 115, 388, 460, 422, 250, 764, 106, 392, 596, 477,\n",
       "        371, 424, 410, 631, 441, 223, 474, 826, 638, 479, 592, 546, 338,\n",
       "        332, 331, 300, 843, 564, 533, 321, 845, 534, 693, 330, 529, 342,\n",
       "        846, 511, 863, 708, 573, 861, 863, 532, 873, 854, 153, 687, 686,\n",
       "        307, 712, 563, 548, 143, 318, 847, 838, 160, 567, 156, 521, 322,\n",
       "        162, 565, 307, 545, 315, 715, 864, 511, 572, 535, 147, 509, 166,\n",
       "        512, 859, 689, 876, 702, 532, 292, 537, 539, 153, 850, 838, 339,\n",
       "        570, 686, 708, 706, 166, 511, 342, 696, 320, 851, 334, 714, 159,\n",
       "        853, 862, 336, 544, 155, 843, 858, 509, 708, 861, 848, 574, 710,\n",
       "        162, 569, 161, 850, 835, 333, 556, 843, 167, 150, 702, 294, 698,\n",
       "        707, 529, 146, 142, 297, 564, 874, 716, 534, 303, 518, 567, 714,\n",
       "        856, 711, 161, 319, 843, 157, 846, 837, 842, 322, 305, 151, 328,\n",
       "        704, 703, 140, 522, 848, 702, 864, 508, 331, 316, 569, 308, 573,\n",
       "        524, 144, 562, 165, 162, 877, 289, 574, 687, 338, 565, 514, 290,\n",
       "        293, 847, 167, 300, 301, 316, 141, 532, 842, 152, 322, 573, 516,\n",
       "        310, 318, 708, 152, 312, 568, 842, 321, 164, 297, 559, 510, 508,\n",
       "        340, 568, 150, 303, 861, 148]),\n",
       " array([620, 730, 705, ..., 907, 584, 604]),\n",
       " array([921,  12, 139,  70, 918, 204, 920,  40, 200,  22,  40, 115, 922,\n",
       "          6, 930, 154,  88,  31, 457, 922,  88, 860,  74,  94, 194,  79,\n",
       "        929,   1, 144,   8,  20, 206, 856, 154, 115,  24, 538, 154, 536,\n",
       "        140, 198,  31, 536, 930,  30, 138, 205, 539, 457,  26, 150,  85,\n",
       "        143, 154,  93, 156,  80, 860,  17, 146,  42,  83,  92,  83,  91,\n",
       "         81,   3, 153,  17,  19, 153,  94, 457,  71,  30, 137, 194, 920,\n",
       "         93, 198,   6,  80, 136, 138,  77,  37, 145,  71,   1,  79,  20,\n",
       "        158, 234,   8, 537, 233,  13,  67,  94,  96,  69,  87, 236, 863,\n",
       "        234,  24, 236,  31, 535, 156, 146,  87,   6,  93,  10, 116,  77,\n",
       "        927, 123,   9, 919, 203, 111, 155,  32, 153,  19,  15,  88,  66,\n",
       "         15, 859, 928, 859,  89,  65, 154,  93,  77, 203, 457, 195, 235,\n",
       "         86, 858, 157, 140, 857, 918, 206, 921, 538, 111, 138, 919,  76,\n",
       "         84, 111, 199,  27, 144,   8, 858,  30,  95,  18,  90,  22, 204,\n",
       "        537, 136, 924, 116, 456,   7,  40, 116,   4,  33, 861, 138,  69,\n",
       "        140,  87, 201,  31, 765, 681, 869, 762, 763, 913, 380, 907, 163,\n",
       "        349,  64, 544, 130, 917, 118, 876, 543, 542, 884, 545, 210, 455,\n",
       "        882,  45, 903, 873,  46, 117, 904, 551, 889, 349, 132, 237, 384,\n",
       "        382,  63,  57, 151, 151, 132,  64, 152, 764, 109, 549, 383, 900,\n",
       "        542, 871, 134, 548, 382, 886, 546, 868, 868,  64, 382, 349, 550,\n",
       "        905, 320, 541, 128, 189, 880, 892, 197, 881, 192, 884, 879, 889,\n",
       "        880, 871, 870, 893, 866, 191, 455,  63, 767, 132, 237, 898, 911,\n",
       "        868, 545, 652, 192, 906, 455, 900,  57, 899, 460, 879, 902, 191,\n",
       "        163, 107, 135,  99, 239, 135, 108, 160, 109, 109, 196, 149, 679,\n",
       "        916,  43, 231, 160, 232, 131, 381,  45, 909, 909, 119, 886, 903,\n",
       "        319, 149, 679, 381, 101, 124, 546,  46, 681, 191, 129, 902, 384,\n",
       "        240, 161, 130, 884,  97, 902, 871, 319, 906, 191, 680, 196, 549,\n",
       "        865, 161, 910, 350, 892, 547, 196, 321, 765, 903,  47, 899, 109,\n",
       "        545, 240, 872, 197,  50, 903, 193, 124, 119, 100, 881, 461, 163,\n",
       "        916, 209, 711, 385, 151, 546, 679, 547, 355, 464, 502, 272, 698,\n",
       "        316, 682, 333, 569, 523, 598, 482, 532, 560, 706, 735, 556, 478,\n",
       "        580, 725, 776, 604, 483, 255, 611, 293, 397, 558,  55, 485, 586,\n",
       "        402, 462, 166, 523, 790, 798, 521, 693, 553, 803, 491, 259, 473,\n",
       "        173, 409, 345, 534, 224, 707, 591, 690, 719, 594, 375, 226, 402,\n",
       "        299, 392, 312, 732, 528, 485, 408, 253, 491, 712, 601, 341, 768,\n",
       "        492, 176, 720, 724, 178, 579, 598, 219, 816, 213, 333, 365, 688,\n",
       "        732, 275, 181, 175, 512, 601, 364, 497, 534, 796, 253, 521, 501,\n",
       "        613, 604, 721, 333, 761, 333, 180, 555, 314, 352, 246, 600, 360,\n",
       "        225,  61, 401, 720, 272, 571, 706, 167, 560, 585, 476, 262, 410,\n",
       "        485, 790, 398, 798, 606, 808, 521, 281, 293, 780, 404, 800, 562,\n",
       "        770, 591, 274, 399, 328, 738, 593, 351, 590, 391, 390, 478, 395,\n",
       "        778, 619, 314, 393, 391, 498, 394, 331, 498, 808, 356, 269, 285,\n",
       "        782, 617, 695, 174, 351, 773, 280, 247, 612, 335, 747, 323, 567,\n",
       "        742, 469, 790, 405, 315, 702, 278, 700, 511, 253, 476, 178, 515,\n",
       "        277, 758, 515, 289, 505, 506, 520, 525, 289, 277, 503, 812, 522,\n",
       "        348, 286, 298, 812, 309, 509, 510, 277, 753, 503, 289, 524, 505,\n",
       "        302, 527, 753, 261, 289, 749, 505, 514, 286, 517, 756, 576, 525,\n",
       "        576, 504, 753, 525, 749, 751, 291, 309, 515, 261, 506, 289, 526,\n",
       "        520, 514, 504, 526, 753, 302, 261, 277, 270, 520, 758, 505, 515,\n",
       "        277, 514, 758, 749, 298, 298, 515, 794, 810, 506, 298, 753, 277,\n",
       "        754, 531, 527, 277, 517, 533, 758, 517, 812, 749, 812, 752, 810,\n",
       "        287, 261, 753, 506, 273, 794, 520, 505, 756, 509, 527, 289, 261,\n",
       "        524, 309, 527, 522, 505, 812, 531, 794, 504, 317, 749, 505, 515,\n",
       "        504, 505, 756, 525, 284, 503, 520, 517, 527, 750, 298, 749, 533,\n",
       "        261, 287, 794, 270, 531, 753, 302, 758, 525, 531, 302, 302, 514,\n",
       "        533, 754, 750, 518, 506, 756, 756, 517, 515, 302, 752, 286, 525,\n",
       "        515, 749, 756, 309, 287, 517, 515, 753, 506, 509, 756, 750, 317,\n",
       "        531, 794, 261, 289, 508, 531, 508, 810, 754, 533, 508, 518, 273,\n",
       "        526, 533, 520, 648, 439, 663, 443, 636, 436, 648, 443, 643, 678,\n",
       "        667, 829, 644, 669, 655, 844, 848, 426, 428, 434, 675, 632, 676,\n",
       "        424, 848, 423, 650, 433, 632, 835, 428, 651, 848, 845, 840, 419,\n",
       "        832, 431, 854, 846, 666, 648, 660, 642, 669, 661, 850, 632, 657,\n",
       "        828, 665, 830, 661, 659, 842, 846, 659, 826, 832, 439, 443, 640,\n",
       "        651, 855, 649, 447, 844, 844, 634, 432, 674, 438, 428, 826, 447,\n",
       "        663, 834, 645, 833, 846, 648, 841, 660, 840, 845, 659, 656, 828,\n",
       "        423, 436, 828, 656, 832, 640, 836, 429, 663, 419, 854, 849, 430,\n",
       "        661, 678, 848, 663, 669, 841, 429, 438, 834, 629, 838, 635, 844,\n",
       "        426, 827, 829, 656, 647, 664, 661, 669, 425, 649, 417, 672, 646,\n",
       "        841, 840, 433, 645, 438, 443, 842, 629, 425, 669, 847, 849, 423,\n",
       "        419, 835, 827, 671, 675, 420, 437, 659, 670, 633, 427, 841, 658,\n",
       "        851, 435, 432, 420, 634, 828, 835, 435, 449, 654, 656, 649, 438,\n",
       "        631, 838, 674, 641, 432, 426, 638, 422, 644, 633, 445, 421, 418,\n",
       "        851, 850, 678, 643, 637, 655, 434]),\n",
       " array([  20,    3, 1001, ...,  948,  971,  747])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.data_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cfda309a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35,  7, 25, 42, 17, 44, 38, 20,  2, 32, 45,  1, 16,  4, 22, 49, 41,\n",
       "       18, 43, 24,  0, 30, 31, 12,  3, 37, 13, 40, 33, 21, 47, 27, 15, 39,\n",
       "       50, 10,  9, 48, 34, 23,  5, 36, 26, 19, 11,  8, 28, 46, 29, 14,  6])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9ab09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "# Tested loss with class weight, but doesn't improve the accuracy\n",
    "\n",
    "from collections import defaultdict\n",
    "cnt_class = defaultdict(int)\n",
    "for x, y in train_generator:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for i, cnt in zip(unique, counts):\n",
    "        cnt_class[i] += cnt\n",
    "cnt_class_np = np.array(list(cnt_class.values()))\n",
    "class_weight = sum(cnt_class_np)/(n_classes * cnt_class_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaba5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 58)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ann_files_train), len(list_ann_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cad716d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/aiot/data/origin_npy/annotations_SC/SC4732EJ-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4371FA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4162EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4271FC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4131EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4412EM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4722EM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4141EU-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4581GM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4272FM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4772GC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4541FA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4562FJ-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4401EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4571FV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4181EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4381FC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4451FY-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4262FC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4261FM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4082EP-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4742EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4022EJ-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4481FV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4752EM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4802GV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7121JE-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7041JO-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7072JA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7082JW-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7152JA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7112JE-Hypnogram.npy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ann_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bcb59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modules.Conv1DAttention2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47dd3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.16749106, 0.1742762 , 0.15472369, 0.16053179, 0.17803392,\n",
       "        0.1649434 ]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1,3000,1))\n",
    "x = tf.convert_to_tensor(x)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f391259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv1d_attention2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_block_34 (conv1d_bloc multiple                  320       \n",
      "_________________________________________________________________\n",
      "conv1d_block_35 (conv1d_bloc multiple                  256       \n",
      "_________________________________________________________________\n",
      "conv1d_block_36 (conv1d_bloc multiple                  480       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_block_37 (conv1d_bloc multiple                  6464      \n",
      "_________________________________________________________________\n",
      "conv1d_block_38 (conv1d_bloc multiple                  31040     \n",
      "_________________________________________________________________\n",
      "conv1d_block_39 (conv1d_bloc multiple                  6464      \n",
      "_________________________________________________________________\n",
      "conv1d_block_40 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_41 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_42 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_43 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "conv1d_block_44 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "conv1d_block_45 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "multihead_attention_6 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "multihead_attention_7 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "multihead_attention_8 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "conv1d_block_46 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_47 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_48 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_49 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  115206    \n",
      "=================================================================\n",
      "Total params: 1,394,022\n",
      "Trainable params: 1,389,094\n",
      "Non-trainable params: 4,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e79fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = BASE_LEARNING_RATE\n",
    "    for _ in range(epoch // 10):\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    optimizer.learning_rate = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f28f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        bs = y_pred.shape[0]\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        #loss = -K.sum(loss, -1)\n",
    "        loss = -K.sum(loss) / bs\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "035a46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_fn = weighted_categorical_crossentropy(weights=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40130015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpt_Advanced_Conv1D', max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed5b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "#if manager.latest_checkpoint:\n",
    "#    ckpt.restore(manager.latest_checkpoint)\n",
    "#    start_epoch = ckpt.step.numpy()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f75d2271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 0--------------------\n",
      "[1162 / 1162] Training loss: 3.180745, Training acc: 0.798\n",
      "Training time: 130.09 sec \n",
      "[515 / 515] test loss: 146.889364, test accuracy: 0.709\n",
      "Eval time: 27.83 sec\n",
      "Saved checkpoint for step 2: ./ckpt_Advanced_Conv1D/ckpt-1\n",
      "-------------------- Epoch 1--------------------\n",
      "[1162 / 1162] Training loss: 3.385102, Training acc: 0.726\n",
      "Training time: 130.57 sec \n",
      "[515 / 515] test loss: 56.402125, test accuracy: 0.661\n",
      "Eval time: 27.20 sec\n",
      "-------------------- Epoch 2--------------------\n",
      "[1162 / 1162] Training loss: 3.588799, Training acc: 0.720\n",
      "Training time: 129.54 sec \n",
      "[515 / 515] test loss: 158.990808, test accuracy: 0.682\n",
      "Eval time: 29.97 sec\n",
      "-------------------- Epoch 3--------------------\n",
      "[1162 / 1162] Training loss: 3.572134, Training acc: 0.792\n",
      "Training time: 129.64 sec \n",
      "[515 / 515] test loss: 153.431744, test accuracy: 0.764\n",
      "Eval time: 29.66 sec\n",
      "Saved checkpoint for step 5: ./ckpt_Advanced_Conv1D/ckpt-2\n",
      "-------------------- Epoch 4--------------------\n",
      "[1162 / 1162] Training loss: 3.701643, Training acc: 0.656\n",
      "Training time: 130.89 sec \n",
      "[515 / 515] test loss: 140.866513, test accuracy: 0.294\n",
      "Eval time: 27.18 sec\n",
      "-------------------- Epoch 5--------------------\n",
      "[1162 / 1162] Training loss: 2.826696, Training acc: 0.732\n",
      "Training time: 130.13 sec \n",
      "[515 / 515] test loss: 149.706985, test accuracy: 0.605\n",
      "Eval time: 27.23 sec\n",
      "-------------------- Epoch 6--------------------\n",
      "[1162 / 1162] Training loss: 3.037148, Training acc: 0.729\n",
      "Training time: 133.61 sec \n",
      "[515 / 515] test loss: 138.163196, test accuracy: 0.700\n",
      "Eval time: 27.26 sec\n",
      "-------------------- Epoch 7--------------------\n",
      "[1162 / 1162] Training loss: 3.168044, Training acc: 0.747\n",
      "Training time: 130.40 sec \n",
      "[515 / 515] test loss: 147.586536, test accuracy: 0.549\n",
      "Eval time: 27.27 sec\n",
      "-------------------- Epoch 8--------------------\n",
      "[1162 / 1162] Training loss: 3.596591, Training acc: 0.784\n",
      "Training time: 130.62 sec \n",
      "[515 / 515] test loss: 152.902590, test accuracy: 0.808\n",
      "Eval time: 27.26 sec\n",
      "Saved checkpoint for step 10: ./ckpt_Advanced_Conv1D/ckpt-3\n",
      "-------------------- Epoch 9--------------------\n",
      "[1162 / 1162] Training loss: 2.846616, Training acc: 0.781\n",
      "Training time: 130.16 sec \n",
      "[515 / 515] test loss: 146.045678, test accuracy: 0.764\n",
      "Eval time: 27.21 sec\n",
      "-------------------- Epoch 10--------------------\n",
      "[1162 / 1162] Training loss: 2.257844, Training acc: 0.825\n",
      "Training time: 130.86 sec \n",
      "[515 / 515] test loss: 143.207449, test accuracy: 0.752\n",
      "Eval time: 27.52 sec\n",
      "-------------------- Epoch 11--------------------\n",
      "[1162 / 1162] Training loss: 2.139117, Training acc: 0.822\n",
      "Training time: 131.48 sec \n",
      "[515 / 515] test loss: 140.499771, test accuracy: 0.728\n",
      "Eval time: 27.55 sec\n",
      "-------------------- Epoch 12--------------------\n",
      "[1162 / 1162] Training loss: 1.963671, Training acc: 0.806\n",
      "Training time: 131.09 sec \n",
      "[515 / 515] test loss: 142.497505, test accuracy: 0.751\n",
      "Eval time: 27.21 sec\n",
      "-------------------- Epoch 13--------------------\n",
      "[1162 / 1162] Training loss: 1.897013, Training acc: 0.828\n",
      "Training time: 131.54 sec \n",
      "[515 / 515] test loss: 143.078949, test accuracy: 0.730\n",
      "Eval time: 27.33 sec\n",
      "-------------------- Epoch 14--------------------\n",
      "[1162 / 1162] Training loss: 1.770894, Training acc: 0.838\n",
      "Training time: 134.40 sec \n",
      "[515 / 515] test loss: 147.320681, test accuracy: 0.777\n",
      "Eval time: 27.00 sec\n",
      "-------------------- Epoch 15--------------------\n",
      "[1162 / 1162] Training loss: 1.832128, Training acc: 0.843\n",
      "Training time: 132.24 sec \n",
      "[515 / 515] test loss: 146.388913, test accuracy: 0.765\n",
      "Eval time: 27.02 sec\n",
      "-------------------- Epoch 16--------------------\n",
      "[1162 / 1162] Training loss: 1.789510, Training acc: 0.855\n",
      "Training time: 131.57 sec \n",
      "[515 / 515] test loss: 143.888225, test accuracy: 0.775\n",
      "Eval time: 27.05 sec\n",
      "-------------------- Epoch 17--------------------\n",
      "[1162 / 1162] Training loss: 1.795538, Training acc: 0.850\n",
      "Training time: 131.90 sec \n",
      "[515 / 515] test loss: 143.025357, test accuracy: 0.770\n",
      "Eval time: 27.09 sec\n",
      "-------------------- Epoch 18--------------------\n",
      "[1162 / 1162] Training loss: 1.793227, Training acc: 0.862\n",
      "Training time: 131.93 sec \n",
      "[515 / 515] test loss: 143.617263, test accuracy: 0.765\n",
      "Eval time: 27.30 sec\n",
      "-------------------- Epoch 19--------------------\n",
      "[1162 / 1162] Training loss: 1.712243, Training acc: 0.864\n",
      "Training time: 131.60 sec \n",
      "[515 / 515] test loss: 147.541390, test accuracy: 0.786\n",
      "Eval time: 27.44 sec\n",
      "-------------------- Epoch 20--------------------\n",
      "[1162 / 1162] Training loss: 1.767459, Training acc: 0.865\n",
      "Training time: 134.90 sec \n",
      "[515 / 515] test loss: 147.154636, test accuracy: 0.776\n",
      "Eval time: 27.59 sec\n",
      "-------------------- Epoch 21--------------------\n",
      "[1162 / 1162] Training loss: 1.734179, Training acc: 0.862\n",
      "Training time: 132.00 sec \n",
      "[515 / 515] test loss: 147.380486, test accuracy: 0.774\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 22--------------------\n",
      "[1162 / 1162] Training loss: 1.725555, Training acc: 0.864\n",
      "Training time: 132.82 sec \n",
      "[515 / 515] test loss: 147.622173, test accuracy: 0.777\n",
      "Eval time: 28.50 sec\n",
      "-------------------- Epoch 23--------------------\n",
      "[1162 / 1162] Training loss: 1.717098, Training acc: 0.866\n",
      "Training time: 132.81 sec \n",
      "[515 / 515] test loss: 148.133404, test accuracy: 0.779\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 24--------------------\n",
      "[1162 / 1162] Training loss: 1.711798, Training acc: 0.868\n",
      "Training time: 132.09 sec \n",
      "[515 / 515] test loss: 148.447418, test accuracy: 0.778\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 25--------------------\n",
      "[1162 / 1162] Training loss: 1.703852, Training acc: 0.869\n",
      "Training time: 131.75 sec \n",
      "[515 / 515] test loss: 148.782920, test accuracy: 0.782\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 26--------------------\n",
      "[1162 / 1162] Training loss: 1.698790, Training acc: 0.871\n",
      "Training time: 131.60 sec \n",
      "[515 / 515] test loss: 149.112377, test accuracy: 0.780\n",
      "Eval time: 27.70 sec\n",
      "-------------------- Epoch 27--------------------\n",
      "[1162 / 1162] Training loss: 1.692117, Training acc: 0.871\n",
      "Training time: 131.58 sec \n",
      "[515 / 515] test loss: 149.409173, test accuracy: 0.781\n",
      "Eval time: 27.60 sec\n",
      "-------------------- Epoch 28--------------------\n",
      "[1162 / 1162] Training loss: 1.688202, Training acc: 0.873\n",
      "Training time: 132.78 sec \n",
      "[515 / 515] test loss: 149.634012, test accuracy: 0.781\n",
      "Eval time: 27.62 sec\n",
      "-------------------- Epoch 29--------------------\n",
      "[1162 / 1162] Training loss: 1.682952, Training acc: 0.873\n",
      "Training time: 132.15 sec \n",
      "[515 / 515] test loss: 149.795998, test accuracy: 0.782\n",
      "Eval time: 27.57 sec\n",
      "-------------------- Epoch 30--------------------\n",
      "[1162 / 1162] Training loss: 1.688241, Training acc: 0.873\n",
      "Training time: 132.32 sec \n",
      "[515 / 515] test loss: 149.915697, test accuracy: 0.790\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 31--------------------\n",
      "[1162 / 1162] Training loss: 1.685705, Training acc: 0.873\n",
      "Training time: 134.26 sec \n",
      "[515 / 515] test loss: 149.983050, test accuracy: 0.790\n",
      "Eval time: 27.41 sec\n",
      "-------------------- Epoch 32--------------------\n",
      "[1162 / 1162] Training loss: 1.684165, Training acc: 0.873\n",
      "Training time: 132.95 sec \n",
      "[515 / 515] test loss: 150.051979, test accuracy: 0.789\n",
      "Eval time: 27.58 sec\n",
      "-------------------- Epoch 33--------------------\n",
      "[1162 / 1162] Training loss: 1.683012, Training acc: 0.873\n",
      "Training time: 132.75 sec \n",
      "[515 / 515] test loss: 150.094447, test accuracy: 0.789\n",
      "Eval time: 28.05 sec\n",
      "-------------------- Epoch 34--------------------\n",
      "[1162 / 1162] Training loss: 1.682109, Training acc: 0.873\n",
      "Training time: 133.57 sec \n",
      "[515 / 515] test loss: 150.119129, test accuracy: 0.789\n",
      "Eval time: 27.71 sec\n",
      "-------------------- Epoch 35--------------------\n",
      "[1162 / 1162] Training loss: 1.681353, Training acc: 0.873\n",
      "Training time: 132.23 sec \n",
      "[515 / 515] test loss: 150.138026, test accuracy: 0.789\n",
      "Eval time: 27.94 sec\n",
      "-------------------- Epoch 36--------------------\n",
      "[1162 / 1162] Training loss: 1.680678, Training acc: 0.873\n",
      "Training time: 132.51 sec \n",
      "[515 / 515] test loss: 150.155933, test accuracy: 0.789\n",
      "Eval time: 27.90 sec\n",
      "-------------------- Epoch 37--------------------\n",
      "[1162 / 1162] Training loss: 1.680049, Training acc: 0.874\n",
      "Training time: 132.29 sec \n",
      "[515 / 515] test loss: 150.171728, test accuracy: 0.789\n",
      "Eval time: 27.82 sec\n",
      "-------------------- Epoch 38--------------------\n",
      "[1162 / 1162] Training loss: 1.679446, Training acc: 0.874\n",
      "Training time: 132.22 sec \n",
      "[515 / 515] test loss: 150.187687, test accuracy: 0.790\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 39--------------------\n",
      "[1162 / 1162] Training loss: 1.678872, Training acc: 0.874\n",
      "Training time: 132.26 sec \n",
      "[515 / 515] test loss: 150.203463, test accuracy: 0.790\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 40--------------------\n",
      "[1162 / 1162] Training loss: 1.677057, Training acc: 0.872\n",
      "Training time: 132.42 sec \n",
      "[515 / 515] test loss: 150.204918, test accuracy: 0.790\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 41--------------------\n",
      "[1162 / 1162] Training loss: 1.676919, Training acc: 0.872\n",
      "Training time: 132.07 sec \n",
      "[515 / 515] test loss: 150.207589, test accuracy: 0.791\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 42--------------------\n",
      "[1162 / 1162] Training loss: 1.676799, Training acc: 0.873\n",
      "Training time: 132.25 sec \n",
      "[515 / 515] test loss: 150.210379, test accuracy: 0.792\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 43--------------------\n",
      "[1162 / 1162] Training loss: 1.676689, Training acc: 0.873\n",
      "Training time: 132.28 sec \n",
      "[515 / 515] test loss: 150.212942, test accuracy: 0.792\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 44--------------------\n",
      "[1162 / 1162] Training loss: 1.676597, Training acc: 0.873\n",
      "Training time: 132.25 sec \n",
      "[515 / 515] test loss: 150.215539, test accuracy: 0.793\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 45--------------------\n",
      "[1162 / 1162] Training loss: 1.676515, Training acc: 0.873\n",
      "Training time: 132.44 sec \n",
      "[515 / 515] test loss: 150.217548, test accuracy: 0.793\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 46--------------------\n",
      "[1162 / 1162] Training loss: 1.676438, Training acc: 0.873\n",
      "Training time: 132.61 sec \n",
      "[515 / 515] test loss: 150.220081, test accuracy: 0.794\n",
      "Eval time: 27.45 sec\n",
      "-------------------- Epoch 47--------------------\n",
      "[1162 / 1162] Training loss: 1.676368, Training acc: 0.873\n",
      "Training time: 133.09 sec \n",
      "[515 / 515] test loss: 150.222086, test accuracy: 0.794\n",
      "Eval time: 27.48 sec\n",
      "-------------------- Epoch 48--------------------\n",
      "[1162 / 1162] Training loss: 1.676304, Training acc: 0.873\n",
      "Training time: 133.23 sec \n",
      "[515 / 515] test loss: 150.224239, test accuracy: 0.794\n",
      "Eval time: 27.49 sec\n",
      "-------------------- Epoch 49--------------------\n",
      "[1162 / 1162] Training loss: 1.676237, Training acc: 0.874\n",
      "Training time: 132.53 sec \n",
      "[515 / 515] test loss: 150.226209, test accuracy: 0.794\n",
      "Eval time: 27.76 sec\n"
     ]
    }
   ],
   "source": [
    "best_test_acc = 0.0\n",
    "for e in range(start_epoch, epochs):\n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    print('-'*20, 'Epoch ' + str(e) + '-'*20)\n",
    "    adjust_learning_rate(optimizer, e)\n",
    "    start = time.time()\n",
    "    for idx, (x, y) in enumerate(train_generator):   \n",
    "        y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss = loss_fn(y_onehot, y_pred)\n",
    "            #loss = loss_fn(y, y_pred)\n",
    "        \n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_loss += loss * y_pred.shape[0]\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(train_generator):\n",
    "            print(\"[%d / %d] Training loss: %.6f, Training acc: %.3f\"%\n",
    "                  (idx+1, len(train_generator), total_loss / total_cnt, correct / total_cnt),end='\\r', flush=True)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    print(\"\")\n",
    "    print(\"Training time: %.2f sec \"%(time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    for idx, (x, y) in enumerate(test_generator):\n",
    "        y_pred = model(x, training=False)\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        #total_loss += loss_fn(y, y_pred).numpy() * y_pred.shape[0]\n",
    "        total_loss += loss_fn(y_onehot, y_pred).numpy() * y_pred.shape[0]\n",
    "            \n",
    "        test_acc = correct / total_cnt\n",
    "        test_loss = total_loss / total_cnt\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(test_generator):\n",
    "            print(\"[%d / %d] test loss: %.6f, test accuracy: %.3f\"%\n",
    "                  (idx+1, len(test_generator), test_loss, test_acc),end='\\r', flush=True)\n",
    "    print(\"\")\n",
    "    print(\"Eval time: %.2f sec\"%(time.time() - start))\n",
    "    ckpt.step.assign_add(1)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "confusion_matrix = np.zeros((n_classes,n_classes))\n",
    "for idx, (x, y) in enumerate(test_generator):\n",
    "    y_pred = model(x, training=False)\n",
    "    y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "    correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "    total_cnt += y_pred.shape[0]\n",
    "    y = tf.cast(y, dtype=tf.int32)    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            confusion_matrix[i,j] += np.sum((y_pred_cls.numpy()==i) * (y.numpy()==j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_classes):\n",
    "    print_ln = \"\"\n",
    "    for j in range(n_classes):\n",
    "        print_ln += \"%.3f \"%(confusion_matrix[i,j] / np.sum(confusion_matrix[i]))\n",
    "        #print_ln += \"%d \"%(confusion_matrix[i,j])\n",
    "    print(print_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(20)\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea077a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.array([[0,0,1],[1,0,0],[0,1,0], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pc.shape[0]\n",
    "x = np.tile(np.expand_dims(pc,1), [1,n,1])\n",
    "y = np.empty((n,n,3))\n",
    "y[:] = np.tile(np.expand_dims(pc,0), [n,1,1])\n",
    "dist = np.sum((x - y) ** 2, axis=2) ** 0.5 # n by n matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de208c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_list = []\n",
    "thr = 2\n",
    "for i in range(1, n):    \n",
    "    included = False\n",
    "    for inst_set in instances_list:\n",
    "        if i in inst_set:\n",
    "            included = True\n",
    "            break\n",
    "    if not included:\n",
    "        close_pts = set()           \n",
    "        q = [i]\n",
    "        while q:\n",
    "            j = q.pop()\n",
    "            #if j in close_pts: continue\n",
    "            new_pts = set(np.where(dist[j,:] < thr)[0])\n",
    "            add_pts = new_pts - close_pts\n",
    "            q += list(add_pts)\n",
    "            close_pts = close_pts.union(add_pts)\n",
    "        \n",
    "        instances_list.append(close_pts)\n",
    "centroids = []\n",
    "for s in instances_list:\n",
    "    cent = np.mean(pc[list(s),:], axis=0)\n",
    "    centroids.append(cent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, instances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1,2,3,4],[5,6,7,8],[4,3,2,1]], [[1,2,3,4],[5,9,7,8],[4,3,2,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5315041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:,0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((x[:,:,0] == 5) * (x[:,:,1] == 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29573dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2,3000))\n",
    "b = np.ones(2)\n",
    "c = 'abc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df['b'] = b\n",
    "df['c'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.random((2,3000))\n",
    "e = np.ones(2)*2\n",
    "f = 'def'\n",
    "df2 = pd.DataFrame(d)\n",
    "df2['b'] = e\n",
    "df2['c'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7846010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb59566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
