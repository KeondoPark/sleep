{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17e3cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-29 11:21:10.059206: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf076324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c1c59162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nets' from '/home/keondopark/sleep/JupyterNotebooks/../nets.py'>"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nets\n",
    "from Data import datagen\n",
    "import importlib \n",
    "import resnet1D_Ahmed\n",
    "importlib.reload(nets)  # Python 3.4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16a9b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9df9c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DATA_PATH = os.path.join('/home','aiot','data','origin_npy')\n",
    "save_signals_path_SC = os.path.join(PROCESSED_DATA_PATH,'signals_SC_filtered')\n",
    "save_annotations_path_SC = os.path.join(PROCESSED_DATA_PATH,'annotations_SC')\n",
    "save_signals_path_ST = os.path.join(PROCESSED_DATA_PATH,'signals_ST_filtered')\n",
    "save_annotations_path_ST = os.path.join(PROCESSED_DATA_PATH,'annotations_ST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_annotations_npy(dirname, filepath):\n",
    "    filename = os.path.basename(filepath)\n",
    "    search_filename = filename.split('-')[0][:-2]\n",
    "    file_list = os.listdir(dirname)\n",
    "    filenames = [file for file in file_list if search_filename in file if file.endswith('.npy')]\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f3d64b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_HT1D = (3000,1)\n",
    "n_classes=6\n",
    "epochs = 1\n",
    "bs = 64\n",
    "BASE_LEARNING_RATE = 1e-3\n",
    "data_ratio = 0.25\n",
    "PREV_CNT = 10\n",
    "list_files_SC = [os.path.join(save_signals_path_SC, f) for f in os.listdir(save_signals_path_SC) if f.endswith('.npy')]\n",
    "list_files_ST = [os.path.join(save_signals_path_ST, f) for f in os.listdir(save_signals_path_ST) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccc54e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = 0.7\n",
    "split_cnt_SC = int(train_test_split * len(list_files_SC))\n",
    "split_cnt_ST = int(train_test_split * len(list_files_ST))\n",
    "\n",
    "list_files_train = []\n",
    "list_files_test = []\n",
    "\n",
    "list_ann_files_train = []\n",
    "list_ann_files_test = []\n",
    "\n",
    "\n",
    "list_files_SC_train = np.random.choice(list_files_SC[:split_cnt_SC], int(0.25 * split_cnt_SC), replace=False)\n",
    "list_files_train += list_files_SC_train.tolist()\n",
    "for f in list_files_SC_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_SC, f)\n",
    "    list_ann_files_train.append(os.path.join(save_annotations_path_SC, ann_file[0]))\n",
    "\n",
    "list_files_test += list_files_SC[split_cnt_SC:]\n",
    "\n",
    "for f in list_files_SC[split_cnt_SC:]:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_SC, f)\n",
    "    list_ann_files_test.append(os.path.join(save_annotations_path_SC, ann_file[0]))\n",
    "\n",
    "\n",
    "list_files_ST_train = np.random.choice(list_files_ST[:split_cnt_ST], int(0.25 * split_cnt_ST), replace=False)\n",
    "list_files_train += list_files_ST_train.tolist()\n",
    "for f in list_files_ST_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_ST, f)\n",
    "    list_ann_files_train.append(os.path.join(save_annotations_path_ST, ann_file[0]))\n",
    "\n",
    "list_files_test += list_files_ST[split_cnt_ST:]\n",
    "for f in list_files_ST[split_cnt_ST:]:\n",
    "    ann_file = match_annotations_npy(save_annotations_path_ST, f)\n",
    "    list_ann_files_test.append(os.path.join(save_annotations_path_ST, ann_file[0]))\n",
    "\n",
    "train_generator = datagen.DataGenerator(list_files_train, list_ann_files_train, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=True)\n",
    "test_generator = datagen.DataGenerator(list_files_test, list_ann_files_test, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False)\n",
    "   \n",
    "train_generator2 = datagen.DataGenerator2(list_files_train, list_ann_files_train, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=True, prev_cnt=PREV_CNT)\n",
    "test_generator2 = datagen.DataGenerator2(list_files_test, list_ann_files_test, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False, prev_cnt=PREV_CNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5ddc8708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cnt_class = defaultdict(int)\n",
    "for x, y in train_generator:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for i, cnt in zip(unique, counts):\n",
    "        cnt_class[i] += cnt\n",
    "cnt_class_np = np.zeros((n_classes,))\n",
    "for i in range(n_classes):\n",
    "    cnt_class_np[i] = cnt_class[i]\n",
    "class_weight = 0.1 * np.ones((n_classes,))\n",
    "class_weight[:n_classes-1] = sum(cnt_class_np[:n_classes-1])/(n_classes * cnt_class_np[:n_classes-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a24734d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([49058.,  5067., 14266.,  3202.,  5844.,   126.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#array([49845.,  5164., 13992.,  2675.,  5113.,   139.])\n",
    "cnt_class_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7672d255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211.921875"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(cnt_class_np)/64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aaba5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 58)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ann_files_train), len(list_ann_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4cad716d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/aiot/data/origin_npy/annotations_SC/SC4261FM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4661EJ-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4722EM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4641EP-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4091EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4742EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4021EH-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4451FY-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4532EV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4042EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4571FV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4502EM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4401EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4732EJ-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4461FA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4591GY-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4381FC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4672GV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4002EC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4481FV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4331FV-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4581GM-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4542FW-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4402EW-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4281GC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_SC/SC4822GC-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7172JA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7101JE-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7152JA-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7121JE-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7192JR-Hypnogram.npy',\n",
       " '/home/aiot/data/origin_npy/annotations_ST/ST7211JJ-Hypnogram.npy']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ann_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1bcb59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nets.Conv1DASPP_single()\n",
    "model2 = nets.Conv1DASPP_multi(batch_size=bs, prev_cnt=PREV_CNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "47dd3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(64, 6), dtype=float32, numpy=\n",
       "array([[0.1711521 , 0.16680464, 0.15964754, 0.16473678, 0.17267556,\n",
       "        0.16498333],\n",
       "       [0.17335169, 0.16803941, 0.15796736, 0.1630061 , 0.17224145,\n",
       "        0.165394  ],\n",
       "       [0.17324673, 0.16878518, 0.16058475, 0.16374314, 0.17055361,\n",
       "        0.16308664],\n",
       "       [0.17326629, 0.16711335, 0.1595646 , 0.16380256, 0.17033017,\n",
       "        0.16592304],\n",
       "       [0.17190821, 0.1685011 , 0.15996855, 0.1631845 , 0.17161298,\n",
       "        0.16482462],\n",
       "       [0.17211924, 0.16867751, 0.15875745, 0.16469459, 0.17101847,\n",
       "        0.16473275],\n",
       "       [0.17242816, 0.16667183, 0.15910983, 0.16427645, 0.17245916,\n",
       "        0.16505457],\n",
       "       [0.1715452 , 0.16834572, 0.15970713, 0.16339435, 0.17142244,\n",
       "        0.16558518],\n",
       "       [0.17294171, 0.16811909, 0.15838183, 0.16340521, 0.17201154,\n",
       "        0.16514057],\n",
       "       [0.17245577, 0.16840912, 0.15901439, 0.16352877, 0.17146091,\n",
       "        0.16513099],\n",
       "       [0.1731981 , 0.16804264, 0.15891795, 0.16374621, 0.1713965 ,\n",
       "        0.16469859],\n",
       "       [0.17299755, 0.16857257, 0.1590455 , 0.16335046, 0.17111893,\n",
       "        0.16491498],\n",
       "       [0.17460833, 0.16782959, 0.16104023, 0.16264619, 0.16884303,\n",
       "        0.16503264],\n",
       "       [0.17219742, 0.16788058, 0.16040011, 0.16289724, 0.17040381,\n",
       "        0.16622083],\n",
       "       [0.17095739, 0.16812003, 0.15954982, 0.16459994, 0.17088929,\n",
       "        0.16588348],\n",
       "       [0.1728497 , 0.16718729, 0.15980166, 0.16112272, 0.17312819,\n",
       "        0.16591042],\n",
       "       [0.17232427, 0.16767322, 0.15969552, 0.16317074, 0.1722814 ,\n",
       "        0.16485487],\n",
       "       [0.17248878, 0.16746531, 0.15871185, 0.1643774 , 0.17132908,\n",
       "        0.1656275 ],\n",
       "       [0.1748721 , 0.16832946, 0.1601613 , 0.16269849, 0.16864425,\n",
       "        0.16529435],\n",
       "       [0.17132586, 0.16864614, 0.1608914 , 0.16315344, 0.17047024,\n",
       "        0.16551293],\n",
       "       [0.17334414, 0.16742003, 0.16027553, 0.16241628, 0.17057489,\n",
       "        0.1659691 ],\n",
       "       [0.17274362, 0.16829908, 0.15883635, 0.16359465, 0.17176092,\n",
       "        0.16476539],\n",
       "       [0.17379567, 0.16782776, 0.1605016 , 0.16359644, 0.16913705,\n",
       "        0.16514151],\n",
       "       [0.17302632, 0.16816834, 0.15770599, 0.16425914, 0.17206213,\n",
       "        0.16477802],\n",
       "       [0.17163585, 0.1680216 , 0.15980719, 0.16219765, 0.17251737,\n",
       "        0.16582038],\n",
       "       [0.17334643, 0.16699277, 0.1589075 , 0.16360259, 0.1729146 ,\n",
       "        0.16423608],\n",
       "       [0.17284107, 0.16776304, 0.15976469, 0.16406989, 0.17013198,\n",
       "        0.16542934],\n",
       "       [0.17193699, 0.1683337 , 0.15941392, 0.16331545, 0.17232202,\n",
       "        0.16467792],\n",
       "       [0.17322414, 0.1675088 , 0.15977055, 0.16470224, 0.17076951,\n",
       "        0.16402477],\n",
       "       [0.169804  , 0.16884808, 0.15883921, 0.16502407, 0.17189305,\n",
       "        0.16559157],\n",
       "       [0.17109655, 0.16922118, 0.1600753 , 0.16473141, 0.17107098,\n",
       "        0.16380459],\n",
       "       [0.17278658, 0.16755903, 0.16022976, 0.16271858, 0.17231569,\n",
       "        0.16439037],\n",
       "       [0.1725342 , 0.16700779, 0.15966612, 0.16477476, 0.17093246,\n",
       "        0.16508469],\n",
       "       [0.1736832 , 0.16735199, 0.16026141, 0.1631923 , 0.1711408 ,\n",
       "        0.16437031],\n",
       "       [0.17326769, 0.16855213, 0.15815912, 0.16475303, 0.17117143,\n",
       "        0.1640966 ],\n",
       "       [0.17048629, 0.16863264, 0.15879713, 0.16387981, 0.17169997,\n",
       "        0.1665041 ],\n",
       "       [0.17296624, 0.1674958 , 0.1591686 , 0.16420406, 0.17123981,\n",
       "        0.16492546],\n",
       "       [0.17133161, 0.16696076, 0.15990376, 0.16528817, 0.170343  ,\n",
       "        0.16617273],\n",
       "       [0.17281137, 0.16740075, 0.16018148, 0.16267732, 0.1728624 ,\n",
       "        0.16406664],\n",
       "       [0.17259638, 0.16664582, 0.16098985, 0.16261622, 0.17251638,\n",
       "        0.16463532],\n",
       "       [0.1745163 , 0.16550294, 0.15928386, 0.1649609 , 0.17093296,\n",
       "        0.16480298],\n",
       "       [0.17408675, 0.16790059, 0.15878218, 0.1629447 , 0.17067726,\n",
       "        0.16560848],\n",
       "       [0.17437358, 0.16690636, 0.16010708, 0.16312933, 0.17025684,\n",
       "        0.16522685],\n",
       "       [0.1724    , 0.16823983, 0.15984094, 0.16439155, 0.17069176,\n",
       "        0.1644359 ],\n",
       "       [0.1739508 , 0.16841444, 0.1594477 , 0.16276248, 0.17161798,\n",
       "        0.16380662],\n",
       "       [0.17177929, 0.1674531 , 0.16045378, 0.16468295, 0.17088465,\n",
       "        0.16474625],\n",
       "       [0.17324705, 0.16740304, 0.16018167, 0.16335551, 0.17125735,\n",
       "        0.16455539],\n",
       "       [0.17279406, 0.16766095, 0.15982354, 0.1635001 , 0.17071502,\n",
       "        0.1655064 ],\n",
       "       [0.17378233, 0.16911165, 0.15899129, 0.16360661, 0.17025524,\n",
       "        0.16425282],\n",
       "       [0.17403345, 0.16663136, 0.15808702, 0.16479579, 0.17081472,\n",
       "        0.16563766],\n",
       "       [0.17369652, 0.16698626, 0.16008908, 0.1638619 , 0.17033753,\n",
       "        0.16502877],\n",
       "       [0.17396812, 0.16678034, 0.15982145, 0.16319178, 0.17179704,\n",
       "        0.16444121],\n",
       "       [0.1728605 , 0.16657318, 0.15938738, 0.16482547, 0.16970064,\n",
       "        0.16665284],\n",
       "       [0.17241423, 0.16850707, 0.16056228, 0.16264313, 0.17047733,\n",
       "        0.16539596],\n",
       "       [0.17158273, 0.16665784, 0.16098936, 0.16315366, 0.17057505,\n",
       "        0.16704135],\n",
       "       [0.1730615 , 0.16878645, 0.1579971 , 0.16296059, 0.1701402 ,\n",
       "        0.16705416],\n",
       "       [0.17215781, 0.16755849, 0.1595953 , 0.16440703, 0.16953993,\n",
       "        0.1667414 ],\n",
       "       [0.17379963, 0.1680223 , 0.15963922, 0.16278791, 0.17043795,\n",
       "        0.16531296],\n",
       "       [0.17241484, 0.1683774 , 0.15963209, 0.16545826, 0.16999495,\n",
       "        0.16412248],\n",
       "       [0.17451991, 0.16904181, 0.15906142, 0.16237092, 0.17052361,\n",
       "        0.16448236],\n",
       "       [0.1718966 , 0.16916741, 0.16005793, 0.16115396, 0.17189138,\n",
       "        0.16583271],\n",
       "       [0.17260681, 0.16879445, 0.15891895, 0.1644841 , 0.17007227,\n",
       "        0.16512345],\n",
       "       [0.17418134, 0.16770998, 0.16066758, 0.16279404, 0.170379  ,\n",
       "        0.16426808],\n",
       "       [0.17239931, 0.16800463, 0.16012105, 0.16393323, 0.17169917,\n",
       "        0.16384263]], dtype=float32)>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((bs,3000,1))\n",
    "x = tf.convert_to_tensor(x)\n",
    "model(x)\n",
    "model2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f391259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv1daspp_single_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1daspp_layer_1 (Conv1DAS multiple                  3379456   \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             multiple                  52230     \n",
      "=================================================================\n",
      "Total params: 3,431,686\n",
      "Trainable params: 3,425,990\n",
      "Non-trainable params: 5,696\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7e79fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = BASE_LEARNING_RATE\n",
    "    for _ in range(epoch // 10):\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    optimizer.learning_rate = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "f28f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        bs = y_pred.shape[0]\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        #loss = -K.sum(loss, -1)\n",
    "        loss = -K.sum(loss) / bs\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "035a46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "#loss_fn = weighted_categorical_crossentropy(weights=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "40130015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpt_Advanced_Conv1D', max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "4ed5b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "#if manager.latest_checkpoint:\n",
    "#    ckpt.restore(manager.latest_checkpoint)\n",
    "#    start_epoch = ckpt.step.numpy()-1\n",
    "best_test_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "76064428",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss_fn(y, y_pred)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))    \n",
    "    return loss_value, y_pred\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    y_pred = model(x, training=False)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "f75d2271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch 0--------------------\n",
      "[1206 / 1206] Training loss: 0.868147, Training acc: 0.805\n",
      "Training loss: 0.868147, Training acc: 0.805\n",
      "Training time: 174.43 sec \n"
     ]
    }
   ],
   "source": [
    "for e in range(start_epoch, epochs):\n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    print('-'*20 + 'Epoch ' + str(e) + '-'*20)\n",
    "    adjust_learning_rate(optimizer, e)\n",
    "    start = time.time()\n",
    "    for idx, (x, y) in enumerate(train_generator):   \n",
    "        #print(x.shape)\n",
    "        #loss, y_pred = train_step(x, y)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss_value = loss_fn(y, y_pred)\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))    \n",
    "        loss = loss_value\n",
    "\n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_loss += loss * y_pred.shape[0]\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(train_generator):\n",
    "            print(\"[%d / %d] Training loss: %.6f, Training acc: %.3f\"%\n",
    "                  (idx+1, len(train_generator), total_loss / total_cnt, correct / total_cnt),end='\\r', flush=True)\n",
    "        \n",
    "    print(\"\")\n",
    "    print(\"Training loss: %.6f, Training acc: %.3f\"%(total_loss / total_cnt, correct / total_cnt))\n",
    "    print(\"Training time: %.2f sec \"%(time.time() - start))\n",
    "    ckpt.step.assign_add(1)\n",
    "    \n",
    "    if e+1 >= 10 and (e+1) % 5 == 0:\n",
    "        start = time.time()\n",
    "        \n",
    "        correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "        for idx, (x, y) in enumerate(test_generator):\n",
    "            #y_pred = model(x, training=False)\n",
    "            \n",
    "            y_pred = test_step(x, y)\n",
    "            y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "            correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "            total_cnt += y_pred.shape[0]\n",
    "            y = tf.cast(y, dtype=tf.int32)\n",
    "            y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "            total_loss += loss_fn(y, y_pred).numpy() * y_pred.shape[0]\n",
    "            #total_loss += loss_fn(y_onehot, y_pred).numpy() * y_pred.shape[0]\n",
    "                \n",
    "            test_acc = correct / total_cnt\n",
    "            test_loss = total_loss / total_cnt\n",
    "            if (idx + 1) % 10 == 0 or idx+1 == len(test_generator):\n",
    "                print(\"[%d / %d] test loss: %.6f, test accuracy: %.3f\"%\n",
    "                    (idx+1, len(test_generator), test_loss, test_acc),end='\\r', flush=True)\n",
    "            \n",
    "        print(\"\")\n",
    "        print(\"test loss: %.6f, test acc: %.3f\"%(test_loss, test_acc))\n",
    "        print(\"Eval time: %.2f sec\"%(time.time() - start))\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6e0bd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.aspp.set_weights(model.aspp.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "d620ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.aspp.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c28d06ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Epoch 0--------------------\n",
      "[1430 / 1206] Training loss: 0.286642, Training acc: 0.788\r"
     ]
    }
   ],
   "source": [
    "for e in range(start_epoch, epochs):\n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    print('-'*20 + 'Epoch ' + str(e) + '-'*20)\n",
    "    adjust_learning_rate(optimizer, e)\n",
    "    start = time.time()\n",
    "    for idx, (x, y, batch_idx) in enumerate(train_generator2):  \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model2(x, training=True)             \n",
    "            loss_value = loss_fn(y, y_pred)\n",
    "        grads = tape.gradient(loss_value, model2.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model2.trainable_weights))   \n",
    "        \n",
    "        total_cnt += y_pred.shape[0] - PREV_CNT\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls[PREV_CNT:], y[PREV_CNT:]), tf.float32))\n",
    "        if batch_idx == 0:\n",
    "            correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls[:PREV_CNT], y[:PREV_CNT]), tf.float32))\n",
    "            total_cnt += PREV_CNT\n",
    "        total_loss += loss * y_pred.shape[0]\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(train_generator2):\n",
    "            print(\"[%d / %d] Training loss: %.6f, Training acc: %.3f\"%\n",
    "                  (idx+1, len(train_generator2), total_loss / total_cnt, correct / total_cnt),end='\\r', flush=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5fe553de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss: 0.217364, Training acc: 0.631\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Training loss: %.6f, Training acc: %.3f\"%(total_loss / total_cnt, correct / total_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "confusion_matrix = np.zeros((n_classes,n_classes))\n",
    "for idx, (x, y) in enumerate(test_generator):\n",
    "    y_pred = model(x, training=False)\n",
    "    y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "    correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "    total_cnt += y_pred.shape[0]\n",
    "    y = tf.cast(y, dtype=tf.int32)    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            confusion_matrix[i,j] += np.sum((y_pred_cls.numpy()==i) * (y.numpy()==j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_classes):\n",
    "    print_ln = \"\"\n",
    "    for j in range(n_classes):\n",
    "        print_ln += \"%.3f \"%(confusion_matrix[i,j] / np.sum(confusion_matrix[i]))\n",
    "        #print_ln += \"%d \"%(confusion_matrix[i,j])\n",
    "    print(print_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(20)\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea077a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.array([[0,0,1],[1,0,0],[0,1,0], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pc.shape[0]\n",
    "x = np.tile(np.expand_dims(pc,1), [1,n,1])\n",
    "y = np.empty((n,n,3))\n",
    "y[:] = np.tile(np.expand_dims(pc,0), [n,1,1])\n",
    "dist = np.sum((x - y) ** 2, axis=2) ** 0.5 # n by n matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de208c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_list = []\n",
    "thr = 2\n",
    "for i in range(1, n):    \n",
    "    included = False\n",
    "    for inst_set in instances_list:\n",
    "        if i in inst_set:\n",
    "            included = True\n",
    "            break\n",
    "    if not included:\n",
    "        close_pts = set()           \n",
    "        q = [i]\n",
    "        while q:\n",
    "            j = q.pop()\n",
    "            #if j in close_pts: continue\n",
    "            new_pts = set(np.where(dist[j,:] < thr)[0])\n",
    "            add_pts = new_pts - close_pts\n",
    "            q += list(add_pts)\n",
    "            close_pts = close_pts.union(add_pts)\n",
    "        \n",
    "        instances_list.append(close_pts)\n",
    "centroids = []\n",
    "for s in instances_list:\n",
    "    cent = np.mean(pc[list(s),:], axis=0)\n",
    "    centroids.append(cent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, instances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1,2,3,4],[5,6,7,8],[4,3,2,1]], [[1,2,3,4],[5,9,7,8],[4,3,2,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5315041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:,0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((x[:,:,0] == 5) * (x[:,:,1] == 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29573dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2,3000))\n",
    "b = np.ones(2)\n",
    "c = 'abc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df['b'] = b\n",
    "df['c'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.random((2,3000))\n",
    "e = np.ones(2)*2\n",
    "f = 'def'\n",
    "df2 = pd.DataFrame(d)\n",
    "df2['b'] = e\n",
    "df2['c'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7846010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb59566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
