{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e3cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 00:55:55.505272: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a9b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fde0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.compat.v2.keras.utils.Sequence):\n",
    "    def __init__(self, data_path, ann_path, list_files, list_ann_files, \n",
    "                 batch_size=64, dim=(3000,1), n_classes=5, shuffle=True):\n",
    "        # Constructor of the data generator.\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.data_path = data_path\n",
    "        self.ann_path = ann_path\n",
    "        self.list_files = list_files\n",
    "        self.list_ann_files = list_ann_files\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.get_cnts() #Get the data count for each file        \n",
    "        self.on_epoch_end() #Initialize file indexes        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch\n",
    "        return int((self.total_len+1) / self.batch_size)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        start = index*self.batch_size\n",
    "        end = min((index+1)*self.batch_size, self.total_len)\n",
    "        \n",
    "        X = np.empty((end - start,) + self.dim, dtype=np.float32)\n",
    "        y = np.empty((end - start,), dtype=np.int32)\n",
    "        \n",
    "        curr_file_idx, accum_start, accum_end = self.get_accum_idx(index)\n",
    "        \n",
    "        curr_file = self.list_files[self.file_indexes[curr_file_idx]]\n",
    "        curr_ann_file = self.list_ann_files[self.file_indexes[curr_file_idx]]\n",
    "        data_index = self.data_indexes[self.file_indexes[curr_file_idx]]\n",
    "        \n",
    "        curr_np = np.load(os.path.join(self.data_path, curr_file))\n",
    "        curr_ann = np.load(os.path.join(self.ann_path, curr_ann_file))\n",
    "        curr_np = curr_np[data_index]\n",
    "        curr_ann = curr_ann[data_index]\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        X_1 = curr_np[start - accum_start:end - accum_start] \n",
    "        y_1 = curr_ann[start - accum_start:end - accum_start]\n",
    "        from_curr = min(accum_end - start, end - start)\n",
    "        X[:from_curr] = np.expand_dims(X_1, axis=-1)\n",
    "        y[:from_curr] = y_1\n",
    "        \n",
    "        if end > accum_end:\n",
    "            curr_file_idx += 1\n",
    "            accum_start = accum_end\n",
    "            accum_end += self.list_cnt[self.file_indexes[curr_file_idx]]\n",
    "            curr_file = self.list_files[self.file_indexes[curr_file_idx]]            \n",
    "            data_index = self.data_indexes[self.file_indexes[curr_file_idx]]\n",
    "            \n",
    "            \n",
    "            curr_ann_file = self.list_ann_files[self.file_indexes[curr_file_idx]]\n",
    "            curr_np = np.load(os.path.join(self.data_path, curr_file))\n",
    "            curr_ann = np.load(os.path.join(self.ann_path, curr_ann_file))\n",
    "\n",
    "            curr_np = curr_np[data_index]\n",
    "            curr_ann = curr_ann[data_index]\n",
    "            #curr_np = curr_np.reshape(-1, 3000, 1)\n",
    "            \n",
    "            #curr_np = curr_np[1:-1]\n",
    "            #curr_ann = curr_ann[1:-1]\n",
    "            \n",
    "            X_2 = curr_np[:end - accum_start]\n",
    "            y_2 = curr_ann[:end - accum_start]\n",
    "            X[from_curr:] = np.expand_dims(X_2, axis=-1)\n",
    "            y[from_curr:] = y_2\n",
    "        \n",
    "        '''\n",
    "        # Normalize data(MinMax)\n",
    "        rng = np.max(X, axis=1) - np.min(X, axis=1) #X shape: (B, 3000, 1), rng: (B, 1)\n",
    "        rng = np.expand_dims(rng, axis=1) #(B, 1, 1)\n",
    "        X = (X - np.expand_dims(np.min(X, axis=1),axis=1)) / (rng + 1e-8)\n",
    "        '''                \n",
    "        return X, y\n",
    "    \n",
    "    def get_accum_idx(self, index):\n",
    "        curr_file_idx = 0\n",
    "        accum_start = 0\n",
    "        accum_end = self.list_cnt[self.file_indexes[0]]\n",
    "        for i in range(len(self.file_indexes)):\n",
    "            if index * self.batch_size < accum_end:\n",
    "                curr_file_idx = i                \n",
    "                break            \n",
    "            accum_start += self.list_cnt[self.file_indexes[i]]\n",
    "            accum_end += self.list_cnt[self.file_indexes[i+1]]\n",
    "        \n",
    "        return curr_file_idx, accum_start, accum_end\n",
    "        \n",
    "    def on_epoch_end(self):        \n",
    "        self.curr_file_idx = 0\n",
    "        # This function is called at the end of each epoch.\n",
    "        self.file_indexes = np.arange(len(self.list_files)) #This is necessary to shuffle files\n",
    "        self.data_indexes = [np.arange(cnt) for cnt in self.list_cnt]\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.file_indexes)\n",
    "            for i in range(len(self.list_cnt)):\n",
    "                np.random.shuffle(self.data_indexes[i]) \n",
    "            \n",
    "        #self.accum_start = 0 \n",
    "        #self.accum_end = self.list_cnt[self.file_indexes[0]]                 \n",
    "            \n",
    "    def get_cnts(self):\n",
    "        list_cnt = []\n",
    "        for f in self.list_files:\n",
    "            temp_np = np.load(os.path.join(self.data_path, f))\n",
    "            cnt_data = temp_np.shape[0] \n",
    "            list_cnt.append(cnt_data)\n",
    "            \n",
    "        self.list_cnt = list_cnt\n",
    "        self.total_len = sum(list_cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df9c4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curr_path = os.getcwd() + '/'\n",
    "PROCESSED_DATA_PATH = os.path.join('/home','aiot','data','origin_npy')\n",
    "save_signals_path = os.path.join(PROCESSED_DATA_PATH,'signals_SC_filtered')\n",
    "save_annotations_path = os.path.join(PROCESSED_DATA_PATH,'annotations_SC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457af95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_annotations_npy(dirname, filename):\n",
    "    search_filename = filename.split('-')[0][:-2]\n",
    "    file_list = os.listdir(dirname)\n",
    "    filenames = [file for file in file_list if search_filename in file if file.endswith('.npy')]\n",
    "\n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d64b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_HT1D = (3000,1)\n",
    "n_classes=6\n",
    "epochs = 50\n",
    "bs = 64\n",
    "BASE_LEARNING_RATE = 1e-3\n",
    "list_files = [f for f in os.listdir(save_signals_path) if f.endswith('.npy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cf21d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_to_list(filepath):\n",
    "    import csv\n",
    "    with open(filepath, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        list_filepath = [row[0] for row in spamreader]\n",
    "    return list_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccc54e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_train = os.path.join('/home','aiot','data','origin_npy','SC_train.csv')\n",
    "SC_test = os.path.join('/home','aiot','data','origin_npy','SC_test.csv')\n",
    "\n",
    "list_files_train = read_csv_to_list(SC_train)\n",
    "list_files_test = read_csv_to_list(SC_test)\n",
    "\n",
    "list_files_train = [f + '.npy' for f in list_files_train]\n",
    "list_files_test = [f + '.npy' for f in list_files_test]\n",
    "\n",
    "list_ann_files_train = []\n",
    "list_ann_files_test = []\n",
    "for f in list_files_train:\n",
    "    ann_file = match_annotations_npy(save_annotations_path, f)\n",
    "    list_ann_files_train.append(ann_file[0])\n",
    "    \n",
    "for f in list_files_test:\n",
    "    ann_file = match_annotations_npy(save_annotations_path, f)\n",
    "    list_ann_files_test.append(ann_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac79f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(save_signals_path, save_annotations_path, list_files_train, list_ann_files_train, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5097eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = DataGenerator(save_signals_path, save_annotations_path, list_files_test, list_ann_files_test, \n",
    "                          batch_size=bs, dim=dim_HT1D, n_classes=n_classes, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ab09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weight\n",
    "# Tested loss with class weight, but doesn't improve the accuracy\n",
    "\n",
    "from collections import defaultdict\n",
    "cnt_class = defaultdict(int)\n",
    "for x, y in train_generator:\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for i, cnt in zip(unique, counts):\n",
    "        cnt_class[i] += cnt\n",
    "cnt_class_np = np.array(list(cnt_class.values()))\n",
    "class_weight = sum(cnt_class_np)/(n_classes * cnt_class_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaba5bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ann_files_train), len(list_ann_files_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cad716d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SC4001EC-Hypnogram.npy',\n",
       " 'SC4002EC-Hypnogram.npy',\n",
       " 'SC4011EH-Hypnogram.npy',\n",
       " 'SC4012EC-Hypnogram.npy',\n",
       " 'SC4021EH-Hypnogram.npy',\n",
       " 'SC4022EJ-Hypnogram.npy',\n",
       " 'SC4031EC-Hypnogram.npy',\n",
       " 'SC4032EP-Hypnogram.npy',\n",
       " 'SC4041EC-Hypnogram.npy',\n",
       " 'SC4042EC-Hypnogram.npy',\n",
       " 'SC4051EC-Hypnogram.npy',\n",
       " 'SC4052EC-Hypnogram.npy',\n",
       " 'SC4061EC-Hypnogram.npy',\n",
       " 'SC4062EC-Hypnogram.npy',\n",
       " 'SC4131EC-Hypnogram.npy',\n",
       " 'SC4141EU-Hypnogram.npy',\n",
       " 'SC4142EU-Hypnogram.npy',\n",
       " 'SC4151EC-Hypnogram.npy',\n",
       " 'SC4152EC-Hypnogram.npy',\n",
       " 'SC4161EC-Hypnogram.npy',\n",
       " 'SC4162EC-Hypnogram.npy',\n",
       " 'SC4171EU-Hypnogram.npy',\n",
       " 'SC4172EC-Hypnogram.npy',\n",
       " 'SC4181EC-Hypnogram.npy',\n",
       " 'SC4182EC-Hypnogram.npy',\n",
       " 'SC4191EP-Hypnogram.npy',\n",
       " 'SC4192EV-Hypnogram.npy']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ann_files_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac46999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'modules' from '/home/keondopark/sleep/modules.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modules\n",
    "import importlib \n",
    "importlib.reload(modules)  # Python 3.4+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bcb59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modules.Conv1DAttention2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47dd3fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6), dtype=float32, numpy=\n",
       "array([[0.16749106, 0.1742762 , 0.15472369, 0.16053179, 0.17803392,\n",
       "        0.1649434 ]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((1,3000,1))\n",
    "x = tf.convert_to_tensor(x)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f391259",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"conv1d_attention2_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_block_34 (conv1d_bloc multiple                  320       \n",
      "_________________________________________________________________\n",
      "conv1d_block_35 (conv1d_bloc multiple                  256       \n",
      "_________________________________________________________________\n",
      "conv1d_block_36 (conv1d_bloc multiple                  480       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_block_37 (conv1d_bloc multiple                  6464      \n",
      "_________________________________________________________________\n",
      "conv1d_block_38 (conv1d_bloc multiple                  31040     \n",
      "_________________________________________________________________\n",
      "conv1d_block_39 (conv1d_bloc multiple                  6464      \n",
      "_________________________________________________________________\n",
      "conv1d_block_40 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_41 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_42 (conv1d_bloc multiple                  25216     \n",
      "_________________________________________________________________\n",
      "conv1d_block_43 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "conv1d_block_44 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "conv1d_block_45 (conv1d_bloc multiple                  99584     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_6 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_8 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "multihead_attention_6 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "multihead_attention_7 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "multihead_attention_8 (Multi multiple                  197376    \n",
      "_________________________________________________________________\n",
      "conv1d_block_46 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_47 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_48 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "conv1d_block_49 (conv1d_bloc multiple                  66816     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  115206    \n",
      "=================================================================\n",
      "Total params: 1,394,022\n",
      "Trainable params: 1,389,094\n",
      "Non-trainable params: 4,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e79fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_lr(epoch):\n",
    "    lr = BASE_LEARNING_RATE\n",
    "    for _ in range(epoch // 10):\n",
    "        lr *= 0.1\n",
    "    return lr\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = get_current_lr(epoch)\n",
    "    optimizer.learning_rate = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f28f59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        bs = y_pred.shape[0]\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        #loss = -K.sum(loss, -1)\n",
    "        loss = -K.sum(loss) / bs\n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "035a46cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "loss_fn = weighted_categorical_crossentropy(weights=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40130015",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, './ckpt_Advanced_Conv1D', max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ed5b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "#if manager.latest_checkpoint:\n",
    "#    ckpt.restore(manager.latest_checkpoint)\n",
    "#    start_epoch = ckpt.step.numpy()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f75d2271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Epoch 0--------------------\n",
      "[1162 / 1162] Training loss: 3.180745, Training acc: 0.798\n",
      "Training time: 130.09 sec \n",
      "[515 / 515] test loss: 146.889364, test accuracy: 0.709\n",
      "Eval time: 27.83 sec\n",
      "Saved checkpoint for step 2: ./ckpt_Advanced_Conv1D/ckpt-1\n",
      "-------------------- Epoch 1--------------------\n",
      "[1162 / 1162] Training loss: 3.385102, Training acc: 0.726\n",
      "Training time: 130.57 sec \n",
      "[515 / 515] test loss: 56.402125, test accuracy: 0.661\n",
      "Eval time: 27.20 sec\n",
      "-------------------- Epoch 2--------------------\n",
      "[1162 / 1162] Training loss: 3.588799, Training acc: 0.720\n",
      "Training time: 129.54 sec \n",
      "[515 / 515] test loss: 158.990808, test accuracy: 0.682\n",
      "Eval time: 29.97 sec\n",
      "-------------------- Epoch 3--------------------\n",
      "[1162 / 1162] Training loss: 3.572134, Training acc: 0.792\n",
      "Training time: 129.64 sec \n",
      "[515 / 515] test loss: 153.431744, test accuracy: 0.764\n",
      "Eval time: 29.66 sec\n",
      "Saved checkpoint for step 5: ./ckpt_Advanced_Conv1D/ckpt-2\n",
      "-------------------- Epoch 4--------------------\n",
      "[1162 / 1162] Training loss: 3.701643, Training acc: 0.656\n",
      "Training time: 130.89 sec \n",
      "[515 / 515] test loss: 140.866513, test accuracy: 0.294\n",
      "Eval time: 27.18 sec\n",
      "-------------------- Epoch 5--------------------\n",
      "[1162 / 1162] Training loss: 2.826696, Training acc: 0.732\n",
      "Training time: 130.13 sec \n",
      "[515 / 515] test loss: 149.706985, test accuracy: 0.605\n",
      "Eval time: 27.23 sec\n",
      "-------------------- Epoch 6--------------------\n",
      "[1162 / 1162] Training loss: 3.037148, Training acc: 0.729\n",
      "Training time: 133.61 sec \n",
      "[515 / 515] test loss: 138.163196, test accuracy: 0.700\n",
      "Eval time: 27.26 sec\n",
      "-------------------- Epoch 7--------------------\n",
      "[1162 / 1162] Training loss: 3.168044, Training acc: 0.747\n",
      "Training time: 130.40 sec \n",
      "[515 / 515] test loss: 147.586536, test accuracy: 0.549\n",
      "Eval time: 27.27 sec\n",
      "-------------------- Epoch 8--------------------\n",
      "[1162 / 1162] Training loss: 3.596591, Training acc: 0.784\n",
      "Training time: 130.62 sec \n",
      "[515 / 515] test loss: 152.902590, test accuracy: 0.808\n",
      "Eval time: 27.26 sec\n",
      "Saved checkpoint for step 10: ./ckpt_Advanced_Conv1D/ckpt-3\n",
      "-------------------- Epoch 9--------------------\n",
      "[1162 / 1162] Training loss: 2.846616, Training acc: 0.781\n",
      "Training time: 130.16 sec \n",
      "[515 / 515] test loss: 146.045678, test accuracy: 0.764\n",
      "Eval time: 27.21 sec\n",
      "-------------------- Epoch 10--------------------\n",
      "[1162 / 1162] Training loss: 2.257844, Training acc: 0.825\n",
      "Training time: 130.86 sec \n",
      "[515 / 515] test loss: 143.207449, test accuracy: 0.752\n",
      "Eval time: 27.52 sec\n",
      "-------------------- Epoch 11--------------------\n",
      "[1162 / 1162] Training loss: 2.139117, Training acc: 0.822\n",
      "Training time: 131.48 sec \n",
      "[515 / 515] test loss: 140.499771, test accuracy: 0.728\n",
      "Eval time: 27.55 sec\n",
      "-------------------- Epoch 12--------------------\n",
      "[1162 / 1162] Training loss: 1.963671, Training acc: 0.806\n",
      "Training time: 131.09 sec \n",
      "[515 / 515] test loss: 142.497505, test accuracy: 0.751\n",
      "Eval time: 27.21 sec\n",
      "-------------------- Epoch 13--------------------\n",
      "[1162 / 1162] Training loss: 1.897013, Training acc: 0.828\n",
      "Training time: 131.54 sec \n",
      "[515 / 515] test loss: 143.078949, test accuracy: 0.730\n",
      "Eval time: 27.33 sec\n",
      "-------------------- Epoch 14--------------------\n",
      "[1162 / 1162] Training loss: 1.770894, Training acc: 0.838\n",
      "Training time: 134.40 sec \n",
      "[515 / 515] test loss: 147.320681, test accuracy: 0.777\n",
      "Eval time: 27.00 sec\n",
      "-------------------- Epoch 15--------------------\n",
      "[1162 / 1162] Training loss: 1.832128, Training acc: 0.843\n",
      "Training time: 132.24 sec \n",
      "[515 / 515] test loss: 146.388913, test accuracy: 0.765\n",
      "Eval time: 27.02 sec\n",
      "-------------------- Epoch 16--------------------\n",
      "[1162 / 1162] Training loss: 1.789510, Training acc: 0.855\n",
      "Training time: 131.57 sec \n",
      "[515 / 515] test loss: 143.888225, test accuracy: 0.775\n",
      "Eval time: 27.05 sec\n",
      "-------------------- Epoch 17--------------------\n",
      "[1162 / 1162] Training loss: 1.795538, Training acc: 0.850\n",
      "Training time: 131.90 sec \n",
      "[515 / 515] test loss: 143.025357, test accuracy: 0.770\n",
      "Eval time: 27.09 sec\n",
      "-------------------- Epoch 18--------------------\n",
      "[1162 / 1162] Training loss: 1.793227, Training acc: 0.862\n",
      "Training time: 131.93 sec \n",
      "[515 / 515] test loss: 143.617263, test accuracy: 0.765\n",
      "Eval time: 27.30 sec\n",
      "-------------------- Epoch 19--------------------\n",
      "[1162 / 1162] Training loss: 1.712243, Training acc: 0.864\n",
      "Training time: 131.60 sec \n",
      "[515 / 515] test loss: 147.541390, test accuracy: 0.786\n",
      "Eval time: 27.44 sec\n",
      "-------------------- Epoch 20--------------------\n",
      "[1162 / 1162] Training loss: 1.767459, Training acc: 0.865\n",
      "Training time: 134.90 sec \n",
      "[515 / 515] test loss: 147.154636, test accuracy: 0.776\n",
      "Eval time: 27.59 sec\n",
      "-------------------- Epoch 21--------------------\n",
      "[1162 / 1162] Training loss: 1.734179, Training acc: 0.862\n",
      "Training time: 132.00 sec \n",
      "[515 / 515] test loss: 147.380486, test accuracy: 0.774\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 22--------------------\n",
      "[1162 / 1162] Training loss: 1.725555, Training acc: 0.864\n",
      "Training time: 132.82 sec \n",
      "[515 / 515] test loss: 147.622173, test accuracy: 0.777\n",
      "Eval time: 28.50 sec\n",
      "-------------------- Epoch 23--------------------\n",
      "[1162 / 1162] Training loss: 1.717098, Training acc: 0.866\n",
      "Training time: 132.81 sec \n",
      "[515 / 515] test loss: 148.133404, test accuracy: 0.779\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 24--------------------\n",
      "[1162 / 1162] Training loss: 1.711798, Training acc: 0.868\n",
      "Training time: 132.09 sec \n",
      "[515 / 515] test loss: 148.447418, test accuracy: 0.778\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 25--------------------\n",
      "[1162 / 1162] Training loss: 1.703852, Training acc: 0.869\n",
      "Training time: 131.75 sec \n",
      "[515 / 515] test loss: 148.782920, test accuracy: 0.782\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 26--------------------\n",
      "[1162 / 1162] Training loss: 1.698790, Training acc: 0.871\n",
      "Training time: 131.60 sec \n",
      "[515 / 515] test loss: 149.112377, test accuracy: 0.780\n",
      "Eval time: 27.70 sec\n",
      "-------------------- Epoch 27--------------------\n",
      "[1162 / 1162] Training loss: 1.692117, Training acc: 0.871\n",
      "Training time: 131.58 sec \n",
      "[515 / 515] test loss: 149.409173, test accuracy: 0.781\n",
      "Eval time: 27.60 sec\n",
      "-------------------- Epoch 28--------------------\n",
      "[1162 / 1162] Training loss: 1.688202, Training acc: 0.873\n",
      "Training time: 132.78 sec \n",
      "[515 / 515] test loss: 149.634012, test accuracy: 0.781\n",
      "Eval time: 27.62 sec\n",
      "-------------------- Epoch 29--------------------\n",
      "[1162 / 1162] Training loss: 1.682952, Training acc: 0.873\n",
      "Training time: 132.15 sec \n",
      "[515 / 515] test loss: 149.795998, test accuracy: 0.782\n",
      "Eval time: 27.57 sec\n",
      "-------------------- Epoch 30--------------------\n",
      "[1162 / 1162] Training loss: 1.688241, Training acc: 0.873\n",
      "Training time: 132.32 sec \n",
      "[515 / 515] test loss: 149.915697, test accuracy: 0.790\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 31--------------------\n",
      "[1162 / 1162] Training loss: 1.685705, Training acc: 0.873\n",
      "Training time: 134.26 sec \n",
      "[515 / 515] test loss: 149.983050, test accuracy: 0.790\n",
      "Eval time: 27.41 sec\n",
      "-------------------- Epoch 32--------------------\n",
      "[1162 / 1162] Training loss: 1.684165, Training acc: 0.873\n",
      "Training time: 132.95 sec \n",
      "[515 / 515] test loss: 150.051979, test accuracy: 0.789\n",
      "Eval time: 27.58 sec\n",
      "-------------------- Epoch 33--------------------\n",
      "[1162 / 1162] Training loss: 1.683012, Training acc: 0.873\n",
      "Training time: 132.75 sec \n",
      "[515 / 515] test loss: 150.094447, test accuracy: 0.789\n",
      "Eval time: 28.05 sec\n",
      "-------------------- Epoch 34--------------------\n",
      "[1162 / 1162] Training loss: 1.682109, Training acc: 0.873\n",
      "Training time: 133.57 sec \n",
      "[515 / 515] test loss: 150.119129, test accuracy: 0.789\n",
      "Eval time: 27.71 sec\n",
      "-------------------- Epoch 35--------------------\n",
      "[1162 / 1162] Training loss: 1.681353, Training acc: 0.873\n",
      "Training time: 132.23 sec \n",
      "[515 / 515] test loss: 150.138026, test accuracy: 0.789\n",
      "Eval time: 27.94 sec\n",
      "-------------------- Epoch 36--------------------\n",
      "[1162 / 1162] Training loss: 1.680678, Training acc: 0.873\n",
      "Training time: 132.51 sec \n",
      "[515 / 515] test loss: 150.155933, test accuracy: 0.789\n",
      "Eval time: 27.90 sec\n",
      "-------------------- Epoch 37--------------------\n",
      "[1162 / 1162] Training loss: 1.680049, Training acc: 0.874\n",
      "Training time: 132.29 sec \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515 / 515] test loss: 150.171728, test accuracy: 0.789\n",
      "Eval time: 27.82 sec\n",
      "-------------------- Epoch 38--------------------\n",
      "[1162 / 1162] Training loss: 1.679446, Training acc: 0.874\n",
      "Training time: 132.22 sec \n",
      "[515 / 515] test loss: 150.187687, test accuracy: 0.790\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 39--------------------\n",
      "[1162 / 1162] Training loss: 1.678872, Training acc: 0.874\n",
      "Training time: 132.26 sec \n",
      "[515 / 515] test loss: 150.203463, test accuracy: 0.790\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 40--------------------\n",
      "[1162 / 1162] Training loss: 1.677057, Training acc: 0.872\n",
      "Training time: 132.42 sec \n",
      "[515 / 515] test loss: 150.204918, test accuracy: 0.790\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 41--------------------\n",
      "[1162 / 1162] Training loss: 1.676919, Training acc: 0.872\n",
      "Training time: 132.07 sec \n",
      "[515 / 515] test loss: 150.207589, test accuracy: 0.791\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 42--------------------\n",
      "[1162 / 1162] Training loss: 1.676799, Training acc: 0.873\n",
      "Training time: 132.25 sec \n",
      "[515 / 515] test loss: 150.210379, test accuracy: 0.792\n",
      "Eval time: 27.61 sec\n",
      "-------------------- Epoch 43--------------------\n",
      "[1162 / 1162] Training loss: 1.676689, Training acc: 0.873\n",
      "Training time: 132.28 sec \n",
      "[515 / 515] test loss: 150.212942, test accuracy: 0.792\n",
      "Eval time: 27.65 sec\n",
      "-------------------- Epoch 44--------------------\n",
      "[1162 / 1162] Training loss: 1.676597, Training acc: 0.873\n",
      "Training time: 132.25 sec \n",
      "[515 / 515] test loss: 150.215539, test accuracy: 0.793\n",
      "Eval time: 27.85 sec\n",
      "-------------------- Epoch 45--------------------\n",
      "[1162 / 1162] Training loss: 1.676515, Training acc: 0.873\n",
      "Training time: 132.44 sec \n",
      "[515 / 515] test loss: 150.217548, test accuracy: 0.793\n",
      "Eval time: 27.53 sec\n",
      "-------------------- Epoch 46--------------------\n",
      "[1162 / 1162] Training loss: 1.676438, Training acc: 0.873\n",
      "Training time: 132.61 sec \n",
      "[515 / 515] test loss: 150.220081, test accuracy: 0.794\n",
      "Eval time: 27.45 sec\n",
      "-------------------- Epoch 47--------------------\n",
      "[1162 / 1162] Training loss: 1.676368, Training acc: 0.873\n",
      "Training time: 133.09 sec \n",
      "[515 / 515] test loss: 150.222086, test accuracy: 0.794\n",
      "Eval time: 27.48 sec\n",
      "-------------------- Epoch 48--------------------\n",
      "[1162 / 1162] Training loss: 1.676304, Training acc: 0.873\n",
      "Training time: 133.23 sec \n",
      "[515 / 515] test loss: 150.224239, test accuracy: 0.794\n",
      "Eval time: 27.49 sec\n",
      "-------------------- Epoch 49--------------------\n",
      "[1162 / 1162] Training loss: 1.676237, Training acc: 0.874\n",
      "Training time: 132.53 sec \n",
      "[515 / 515] test loss: 150.226209, test accuracy: 0.794\n",
      "Eval time: 27.76 sec\n"
     ]
    }
   ],
   "source": [
    "best_test_acc = 0.0\n",
    "for e in range(start_epoch, epochs):\n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    print('-'*20, 'Epoch ' + str(e) + '-'*20)\n",
    "    adjust_learning_rate(optimizer, e)\n",
    "    start = time.time()\n",
    "    for idx, (x, y) in enumerate(train_generator):   \n",
    "        y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss = loss_fn(y_onehot, y_pred)\n",
    "            #loss = loss_fn(y, y_pred)\n",
    "        \n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_loss += loss * y_pred.shape[0]\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(train_generator):\n",
    "            print(\"[%d / %d] Training loss: %.6f, Training acc: %.3f\"%\n",
    "                  (idx+1, len(train_generator), total_loss / total_cnt, correct / total_cnt),end='\\r', flush=True)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    print(\"\")\n",
    "    print(\"Training time: %.2f sec \"%(time.time() - start))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "    for idx, (x, y) in enumerate(test_generator):\n",
    "        y_pred = model(x, training=False)\n",
    "        y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "        total_cnt += y_pred.shape[0]\n",
    "        y = tf.cast(y, dtype=tf.int32)\n",
    "        y_onehot = tf.one_hot(y, depth=n_classes)\n",
    "        #total_loss += loss_fn(y, y_pred).numpy() * y_pred.shape[0]\n",
    "        total_loss += loss_fn(y_onehot, y_pred).numpy() * y_pred.shape[0]\n",
    "            \n",
    "        test_acc = correct / total_cnt\n",
    "        test_loss = total_loss / total_cnt\n",
    "        if (idx + 1) % 10 == 0 or idx+1 == len(test_generator):\n",
    "            print(\"[%d / %d] test loss: %.6f, test accuracy: %.3f\"%\n",
    "                  (idx+1, len(test_generator), test_loss, test_acc),end='\\r', flush=True)\n",
    "    print(\"\")\n",
    "    print(\"Eval time: %.2f sec\"%(time.time() - start))\n",
    "    ckpt.step.assign_add(1)\n",
    "    if test_acc > best_test_acc:\n",
    "        best_test_acc = test_acc\n",
    "        save_path = manager.save()\n",
    "        print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step), save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct, total_cnt, total_loss = 0.0, 0.0, 0.0\n",
    "confusion_matrix = np.zeros((n_classes,n_classes))\n",
    "for idx, (x, y) in enumerate(test_generator):\n",
    "    y_pred = model(x, training=False)\n",
    "    y_pred_cls = tf.math.argmax(y_pred, axis=-1)\n",
    "    correct += tf.reduce_sum(tf.cast(tf.equal(y_pred_cls, y), tf.float32))\n",
    "    total_cnt += y_pred.shape[0]\n",
    "    y = tf.cast(y, dtype=tf.int32)    \n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            confusion_matrix[i,j] += np.sum((y_pred_cls.numpy()==i) * (y.numpy()==j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabe1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_classes):\n",
    "    print_ln = \"\"\n",
    "    for j in range(n_classes):\n",
    "        print_ln += \"%.3f \"%(confusion_matrix[i,j] / np.sum(confusion_matrix[i]))\n",
    "        #print_ln += \"%d \"%(confusion_matrix[i,j])\n",
    "    print(print_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(10)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184e0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc3c92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.arange(20)\n",
    "np.random.shuffle(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea077a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = np.array([[0,0,1],[1,0,0],[0,1,0], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = pc.shape[0]\n",
    "x = np.tile(np.expand_dims(pc,1), [1,n,1])\n",
    "y = np.empty((n,n,3))\n",
    "y[:] = np.tile(np.expand_dims(pc,0), [n,1,1])\n",
    "dist = np.sum((x - y) ** 2, axis=2) ** 0.5 # n by n matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de208c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_list = []\n",
    "thr = 2\n",
    "for i in range(1, n):    \n",
    "    included = False\n",
    "    for inst_set in instances_list:\n",
    "        if i in inst_set:\n",
    "            included = True\n",
    "            break\n",
    "    if not included:\n",
    "        close_pts = set()           \n",
    "        q = [i]\n",
    "        while q:\n",
    "            j = q.pop()\n",
    "            #if j in close_pts: continue\n",
    "            new_pts = set(np.where(dist[j,:] < thr)[0])\n",
    "            add_pts = new_pts - close_pts\n",
    "            q += list(add_pts)\n",
    "            close_pts = close_pts.union(add_pts)\n",
    "        \n",
    "        instances_list.append(close_pts)\n",
    "centroids = []\n",
    "for s in instances_list:\n",
    "    cent = np.mean(pc[list(s),:], axis=0)\n",
    "    centroids.append(cent)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d329b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, instances_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1,2,3,4],[5,6,7,8],[4,3,2,1]], [[1,2,3,4],[5,9,7,8],[4,3,2,1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5315041",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,:,0] == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum((x[:,:,0] == 5) * (x[:,:,1] == 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29573dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8220e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((2,3000))\n",
    "b = np.ones(2)\n",
    "c = 'abc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946fba0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(a)\n",
    "df['b'] = b\n",
    "df['c'] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c11101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.random.random((2,3000))\n",
    "e = np.ones(2)*2\n",
    "f = 'def'\n",
    "df2 = pd.DataFrame(d)\n",
    "df2['b'] = e\n",
    "df2['c'] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7846010",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb59566",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
